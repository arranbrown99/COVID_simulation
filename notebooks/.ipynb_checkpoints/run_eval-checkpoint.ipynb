{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "\n",
    "def get_current_dir():\n",
    "    return os.getcwd().split(\"/\")[-1]\n",
    "\n",
    "def in_project_root_directory():\n",
    "    return get_current_dir() == \"ai-coursework-group6\"\n",
    "\n",
    "if not in_project_root_directory():\n",
    "    os.chdir('..')\n",
    "import virl\n",
    "from helper_methods import plot as rand_deter_plot\n",
    "from plot_helper import plot as eval_plot\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from notebooks.eval_data import EvalData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeterministicAgent import DeterministicAgent\n",
    "from helper_methods import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actions = [\"no intervention\", \"impose a full lockdown\", \"implement track and trace\", \"enforce social distancing and face masks\"]\n",
    "\n",
    "def evaluate_deterministic(eval_data, action):\n",
    "    for i in range(0,10):\n",
    "        env = virl.Epidemic(stochastic=eval_data.stochastic, noisy=eval_data.noisy, problem_id=i)\n",
    "        agent = DeterministicAgent(env, action, actions[action])\n",
    "        rewards = run(agent, 25)\n",
    "        eval_data.add_rewards(i, rewards)\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "\n",
    "eval_data = EvalData(\"Deterministic \" + actions[action], stochastic=False, noisy=False)\n",
    "evaluate_deterministic(eval_data, action)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandomAgent import RandomAgent\n",
    "from helper_methods import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"no intervention\", \"impose a full lockdown\", \"implement track & trace\", \"enforce social distancing and face masks\"]\n",
    "\n",
    "def evaluate_random(eval_data):\n",
    "    for i in range(0,10):\n",
    "        env = virl.Epidemic(stochastic=eval_data.stochastic, noisy=eval_data.noisy, problem_id=i)\n",
    "        agent = RandomAgent(env, actions)\n",
    "        rewards = run(agent, 25)\n",
    "        eval_data.add_rewards(i, rewards)\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Random\", stochastic=False, noisy=False)\n",
    "evaluate_random(eval_data)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Random\", stochastic=True, noisy=False)\n",
    "evaluate_random(eval_data)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Random\", stochastic=False, noisy=True)\n",
    "evaluate_random(eval_data)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Random\", stochastic=True, noisy=True)\n",
    "evaluate_random(eval_data)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LinearAprxAgent import LinearAprxAgent\n",
    "\n",
    "def evaluate_linear_policy(eval_data, agent_trained_under_problem_id):\n",
    "    for i in range(0, 10):\n",
    "        #load from disk\n",
    "        intercept = np.genfromtxt(\"trained_agents/linear_policy/interceptP\" + str(agent_trained_under_problem_id) + \".csv\", delimiter=',')\n",
    "        coeff = np.genfromtxt(\"trained_agents/linear_policy/coeffP\" + str(agent_trained_under_problem_id) + \".csv\", delimiter=',')\n",
    "        state_transformed = np.genfromtxt(\"trained_agents/linear_policy/state_transformedP\" + str(agent_trained_under_problem_id) + \".csv\", delimiter=',')\n",
    "        q_values = np.genfromtxt(\"trained_agents/linear_policy/q_valueP\" + str(agent_trained_under_problem_id) + \".csv\", delimiter=',')\n",
    "        last_reward = np.genfromtxt(\"trained_agents/linear_policy/lastRewardP\" + str(agent_trained_under_problem_id) + \".csv\", delimiter=',')\n",
    "\n",
    "        env = virl.Epidemic(stochastic=eval_data.stochastic, noisy=eval_data.noisy)\n",
    "        agent = LinearAprxAgent(env)\n",
    "        states,all_rewards, all_total_rewards = agent.evaluate(intercept,coeff,state_transformed[1:,:],q_values,last_reward,episodes=25)\n",
    "        eval_data.add_rewards(i, all_total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Linear Policy\", stochastic=False, noisy=False)\n",
    "evaluate_linear_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Linear Policy\", stochastic=True, noisy=False)\n",
    "evaluate_linear_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Linear Policy\", stochastic=False, noisy=True)\n",
    "evaluate_linear_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Linear Policy\", stochastic=True, noisy=True)\n",
    "evaluate_linear_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tabular_Policy_Agent import Tabular_Policy_Agent\n",
    "\n",
    "\n",
    "def evaluate_tabular_policy(eval_data, agent_trained_under_problem_id):\n",
    "    for i in range(0, 10):\n",
    "         #load from disk    \n",
    "        \n",
    "        filename = \"trained_agents/tabular_policy/tabular_problem_id_\" + str(agent_trained_under_problem_id) + \".txt\"    \n",
    "        \n",
    "        env = virl.Epidemic(stochastic=eval_data.stochastic, noisy=eval_data.noisy)\n",
    "        agent = Tabular_Policy_Agent(env)\n",
    "        states,all_rewards, all_total_rewards = agent.evaluate(filename,episodes=25)\n",
    "        eval_data.add_rewards(i, all_total_rewards)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Tabular Policy\", stochastic=False, noisy=False)\n",
    "evaluate_tabular_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Tabular Policy\", stochastic=False, noisy=True)\n",
    "evaluate_tabular_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Tabular Policy\", stochastic=True, noisy=False)\n",
    "evaluate_tabular_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Tabular Policy\", stochastic=True, noisy=True)\n",
    "evaluate_tabular_policy(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_tabular.q_table import QTable\n",
    "from q_learning_tabular.q_learning_agent import QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q_learning_tabular(eval_data, agent_trained_under_problem_id):\n",
    "    for i in range(0,10):\n",
    "        filename = \"trained_agents/q_learning_tabular_problem_id_\" + str(agent_trained_under_problem_id) + \".txt\"\n",
    "        internal_loaded_q_table = QTable.load_raw_q_table_from_file(filename)\n",
    "        env = virl.Epidemic(stochastic=eval_data.stochastic, noisy=eval_data.noisy, problem_id=i)\n",
    "        agent = QLearningAgent(env, print_out_every_x_episodes=1, internal_q_table=internal_loaded_q_table)\n",
    "        all_total_rewards = agent.evaluate(25)\n",
    "        eval_data.add_rewards(i, all_total_rewards)\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Tabular\", stochastic=False, noisy=False)\n",
    "evaluate_q_learning_tabular(eval_data, 0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Tabular\", stochastic=True, noisy=False)\n",
    "evaluate_q_learning_tabular(eval_data, 0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Tabular\", stochastic=False, noisy=True)\n",
    "evaluate_q_learning_tabular(eval_data, 0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Tabular\", stochastic=True, noisy=True)\n",
    "evaluate_q_learning_tabular(eval_data, 0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_nn.nn_function_approximator import NNFunctionApproximatorJointKeras, init_networks, load_trained_network\n",
    "from q_learning_nn.memory import ReplayMemory, Transition\n",
    "from q_learning_nn.agent import Agent\n",
    "from q_learning_nn.strategy import Strategy\n",
    "from q_learning_nn.run import qlearning_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q_learning_nn(eval_data, agent_trained_under_problem_id):\n",
    "    for i in range(0,10):\n",
    "        filename = \"policy_network_problem_id_\" + str(agent_trained_under_problem_id) + \".h5\"\n",
    "        policy_network_new, target_network_new = load_trained_network(filename, virl)\n",
    "        agent = Agent(virl.Epidemic(stochastic=eval_data.stochastic, noisy=eval_data.noisy, problem_id=i), learning_rate=0.0)\n",
    "        rewards = qlearning_nn(agent=agent, policy_network=policy_network_new, target_network=target_network_new, num_episodes=25)\n",
    "        eval_data.add_rewards(i, rewards)\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Neural Network\", stochastic=False, noisy=False)\n",
    "evaluate_q_learning_nn(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Neural Network\", stochastic=True, noisy=False)\n",
    "evaluate_q_learning_nn(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Neural Network\", stochastic=False, noisy=True)\n",
    "evaluate_q_learning_nn(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = EvalData(\"Q Learning Neural Network\", stochastic=True, noisy=True)\n",
    "evaluate_q_learning_nn(eval_data, agent_trained_under_problem_id=0)\n",
    "eval_data.create_plot()\n",
    "eval_data.create_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
