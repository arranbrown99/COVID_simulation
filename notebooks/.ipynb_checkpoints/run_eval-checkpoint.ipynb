{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from helper_methods import run, plot\n",
    "from plot_helper import plot as eval_plot\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def plot(all_rewards, title):\n",
    "    plt.title(\"Evaluate: \" + title)\n",
    "    plt.xlabel(\"Epsiode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.plot(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the stochastic and noisy parameters\n",
    "stochastic_noisy = {0:virl.Epidemic(stochastic=False, noisy=False),1:virl.Epidemic(stochastic=True, noisy=False),2:virl.Epidemic(stochastic=False, noisy=True),3:virl.Epidemic(stochastic=True, noisy=True)}\n",
    "stochastic_noisy_print = {0:\"stochastic=False, noisy=False\",1:\"stochastic=True, noisy=False\",2:\"stochastic=False, noisy=True\",3:\"stochastic=True, noisy=True\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std(all_total_rewards,all_rewards,title):\n",
    "    #mean and standard deviation\n",
    "    total_reward_sd = np.std(all_total_rewards)\n",
    "    total_reward_mean = np.mean(all_total_rewards)\n",
    "    reward_sd = np.std(all_rewards)\n",
    "    reward_mean = np.mean(all_rewards)\n",
    "\n",
    "    print(\"---\")\n",
    "    print(title)\n",
    "    print(\"Total reward standard deviation = \" + str(total_reward_sd))\n",
    "    print(\"Total reward mean = \" + str(total_reward_mean))\n",
    "    print(\"Reward standard deviation = \" + str(reward_sd))\n",
    "    print(\"Rotal reward mean = \" + str(reward_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeterministicAgent import DeterministicAgent\n",
    "\n",
    "actions = [\"no intervention\", \"impose a full lockdown\", \"implement track & trace\", \"enforce social distancing and face masks\"]\n",
    "stochastic = [False, True]\n",
    "noisy = [False, True]\n",
    "\n",
    "\n",
    "for i in range(len(stochastic)):\n",
    "    for j in range(len(noisy)):\n",
    "        for action, action_text in enumerate(actions):\n",
    "            env = virl.Epidemic(stochastic=stochastic[i], noisy=noisy[j])\n",
    "            agent = DeterministicAgent(env, action, action_text)\n",
    "            states, rewards = run(agent)\n",
    "            title = \", Stochastic = \" + str(stochastic[i]) + \", \" \"Noisy = \" + str(noisy[j])\n",
    "            plot(agent, states, rewards, title)\n",
    "\n",
    "for i in range(0,10):\n",
    "    for action, action_text in enumerate(actions):\n",
    "        env = virl.Epidemic(stochastic=True, noisy=True,problem_id=i)\n",
    "        agent = DeterministicAgent(env, action, action_text)\n",
    "        states, rewards = run(agent)\n",
    "        title = \", Problem id=\" + str(i)\n",
    "        plot(agent, states, rewards, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandomAgent import RandomAgent\n",
    "\n",
    "actions = [\"no intervention\", \"impose a full lockdown\", \"implement track & trace\", \"enforce social distancing and face masks\"]\n",
    "stochastic = [False, True]\n",
    "noisy = [False, True]\n",
    "\n",
    "for i in range(len(stochastic)):\n",
    "    for j in range(len(noisy)):\n",
    "        env = virl.Epidemic(stochastic=stochastic[i], noisy=noisy[j])\n",
    "        agent = RandomAgent(env, actions)\n",
    "        states, rewards = run(agent)\n",
    "        plot(agent, states, rewards)\n",
    "        \n",
    "for i in range(0,10):\n",
    "    env = virl.Epidemic(stochastic=True, noisy=True,problem_id=i)\n",
    "    agent = RandomAgent(env, actions)\n",
    "    states, rewards = run(agent)\n",
    "    title = \", Problem id=\" + str(i)\n",
    "    plot(agent, states, rewards, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd\n",
    "%cd ai-coursework-group6\\notebooks\\linear_aprx_output\n",
    "%pwd\n",
    "\n",
    "for i in range(0,10):\n",
    "    title = \"Linear Policy Evaluate: problem id \" + str(i) + \" \"\n",
    "    \n",
    "    intercept = np.genfromtxt(\"interceptP\" + str(i) + \".csv\", delimiter=',')\n",
    "    coeff = np.genfromtxt(\"coeffP\" + str(i) + \".csv\", delimiter=',')\n",
    "    state_transformed = np.genfromtxt(\"state_transformedP\" + str(i) + \".csv\", delimiter=',')\n",
    "    q_value = np.genfromtxt(\"q_valueP\" + str(i) + \".csv\", delimiter=',')\n",
    "    \n",
    "    states,all_rewards, all_total_rewards,func_approximator, state_transformed, q_value = agent.evaluate()\n",
    "    eval_plot(states,agent, all_total_rewards, all_rewards,title)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    print(stochastic_noisy_print[i])\n",
    "    title = \"Linear Policy Evaluate:\" + stochastic_noisy_print\n",
    "    \n",
    "    intercept = np.genfromtxt(\"interceptP\" + str(i) + \".csv\", delimiter=',')\n",
    "    coeff = np.genfromtxt(\"coeffP\" + str(i) + \".csv\", delimiter=',')\n",
    "    state_transformed = np.genfromtxt(\"state_transformedP\" + str(i) + \".csv\", delimiter=',')\n",
    "    q_value = np.genfromtxt(\"q_valueP\" + str(i) + \".csv\", delimiter=',')\n",
    "    \n",
    "    states,all_rewards, all_total_rewards,func_approximator, state_transformed, q_value = agent.evaluate()\n",
    "    eval_plot(states,agent, all_total_rewards, all_rewards,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd\n",
    "%cd ai-coursework-group6\\notebooks\n",
    "%pwd\n",
    "\n",
    "for i in range(0,10):\n",
    "    title = \"Q Learning Evaluate: problem id \" + str(i) + \" \"\n",
    "    \n",
    "    with open('qlearningP' + str(i) + '.pkl', 'rb') as inpt:\n",
    "        agent = pickle.load(inpt)\n",
    "        states,all_rewards, all_total_rewards = agent.evaluate()\n",
    "        eval_plot(states,agent, all_total_rewards, all_rewards,title)\n",
    "        mean_std(all_total_rewards,all_rewards,title)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,4):\n",
    "    print(stochastic_noisy_print[i])\n",
    "    title = \"Q Learning Evaluate:\" + stochastic_noisy_print\n",
    "    \n",
    "    with open('qlearningS' + str(i) + '.pkl', 'rb') as inpt:\n",
    "        agent = pickle.load(inpt)\n",
    "    states,all_rewards, all_total_rewards = agent.evaluate()\n",
    "    eval_plot(states,agent, all_total_rewards, all_rewards,title)\n",
    "    mean_std(all_total_rewards,all_rewards,title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_tabular.q_table import QTable\n",
    "from q_learning_tabular.q_learning_agent import QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_q_learning_tabular(stochastic, noisy):\n",
    "    for i in range(0,10):\n",
    "        filename = \"q_learning_tabular_problem_id_\" + str(i) + \".txt\"\n",
    "        internal_loaded_q_table = QTable.load_raw_q_table_from_file(filename)\n",
    "\n",
    "        env = virl.Epidemic(stochastic=stochastic, noisy=noisy, problem_id=i)\n",
    "        agent = QLearningAgent(env, print_out_every_x_episodes=1, internal_q_table=internal_loaded_q_table, )\n",
    "        all_total_rewards = agent.evaluate(10)\n",
    "\n",
    "        title = \"Q Learning Tabular with problem id \" + str(i)\n",
    "        plot(all_total_rewards, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"ai-coursework-group6\")\n",
    "\n",
    "evaluate_q_learning_tabular(stochastic=False, noisy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q learning Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_learning_nn.nn_function_approximator import NNFunctionApproximatorJointKeras, init_networks, load_trained_network\n",
    "from q_learning_nn.memory import ReplayMemory, Transition\n",
    "from q_learning_nn.agent import Agent\n",
    "from q_learning_nn.strategy import Strategy\n",
    "from q_learning_nn.run import qlearning_nn\n",
    "\n",
    "policy_trained_using_problem_id_zero = \"policy_network2.h5\"\n",
    "policy_network_new, target_network_new = load_trained_network(policy_trained_using_problem_id_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    agent = Agent(virl.Epidemic(stochastic=False, noisy=False, problem_id=i), learning_rate=0.0)\n",
    "    rewards = qlearning_nn(\n",
    "        agent=agent, policy_network=policy_network_new, target_network=target_network_new, num_episodes=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
