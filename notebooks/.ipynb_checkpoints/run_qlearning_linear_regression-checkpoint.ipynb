{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and backend for neural networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.99847798e+08 8.92380090e+04 1.95993039e+04 4.33649465e+04]\n",
      "(4,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_10 is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: []\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0ef13d5fbe01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m#update q table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-0ef13d5fbe01>\u001b[0m in \u001b[0;36mupdate2\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#         ValueError: Input 0 of layer sequential_6 is incompatible with the layer: expected axis -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# of input shape to have value 4 but received input with shape [4, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvirl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEpidemic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstochastic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                                     class_weight)\n\u001b[1;32m   1694\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:747 train_step\n        y_pred = self(x, training=True)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:196 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer sequential_10 is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: []\n"
     ]
    }
   ],
   "source": [
    "def model():\n",
    "    # Set up the model\n",
    "    regression = linear_model.LinearRegression() \n",
    "\n",
    "    # Partition data into train and test\n",
    "    train_x = \n",
    "    train_y = \n",
    "    \n",
    "    # Train model\n",
    "    regression.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "\n",
    "epislon = 1\n",
    "rewards = []\n",
    "done = False\n",
    "state = env.reset()\n",
    "\n",
    "while not done:\n",
    "    random_number = np.random.random()\n",
    "    if random_number < epislon:\n",
    "        #explore\n",
    "        action = np.random.choice(n_actions)\n",
    "    else:\n",
    "        #exploit\n",
    "        action = nn.predict(state)\n",
    "        # get prediction here\n",
    "\n",
    "    new_state, reward, done, i = env.step(action=action) # Q-learning\n",
    "\n",
    "    #update q table\n",
    "    nn.update2(new_state)\n",
    "\n",
    "    rewards.append(reward)\n",
    "    state = new_state\n",
    "    epsilon -= 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.num_of_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.q_table = QTable(initial=0, num_of_actions=self.num_of_actions) \n",
    "        \n",
    "        # hyper parameters\n",
    "        self.discount = 0.99 # gamma\n",
    "        self.learning_rate = 0.25 # step size, alpha\n",
    "        self.episodes = 2000\n",
    "        self.print_out_every_x_episodes = int(self.episodes/50)\n",
    "        \n",
    "        # hyper parameters for epsilon\n",
    "        self.initial_epsilon = 1 # initial\n",
    "        self.decrease_factor = (1/self.episodes)/1.25 # epsilon\n",
    "        self.decrease_factor = 0.00075\n",
    "        \n",
    "        print(\"Hyperparameter dump\")\n",
    "        print(\"----\")\n",
    "        print(\"Number Of Episodes = \" + str(self.episodes))\n",
    "        print(\"Print out every \" + str(self.print_out_every_x_episodes) + \" episodes\")\n",
    "        print(\"Learning Rate = \" + str(self.learning_rate))\n",
    "        print(\"Discount = \" + str(self.discount))\n",
    "        print(\"----\")\n",
    "        print(\"Initial Epsilon = \" + str(self.initial_epsilon))\n",
    "        print(\"Epsilon Decrease Factor = \" + str(self.decrease_factor))\n",
    "        print(\"----\")\n",
    "        print(\"Number of Bins to Discretise State = \" + str(self.number_bins))\n",
    "        print(\"----\")\n",
    "    \n",
    "    def run_all_episodes(self):\n",
    "        all_rewards = []\n",
    "        all_q_table_exploits = []\n",
    "        epislon = self.initial_epsilon # at the start only explore\n",
    "        \n",
    "        for episode in range(1, self.episodes + 1):\n",
    "            rewards, exploited_q_table = self.run_episode(epislon)\n",
    "            total_reward = np.sum(rewards)\n",
    "\n",
    "            if episode % self.print_out_every_x_episodes == 0:\n",
    "                print(\"Episode number: \" + str(episode) + \". Total reward in episode: \" + str(total_reward) + \". Episode executed with epsilon = \" + str(epislon))\n",
    "                print(\"Average total reward in last \" + str(self.print_out_every_x_episodes) + \" episodes: \" + str(np.mean(all_rewards[-self.print_out_every_x_episodes:])))\n",
    "                print(\"Average number of times we exploited q table in last \" + str(self.print_out_every_x_episodes) + \" episodes: \" + str(np.mean(all_q_table_exploits[-self.print_out_every_x_episodes:])))\n",
    "                print(\"-----\")\n",
    "            all_rewards.append(total_reward)\n",
    "            all_q_table_exploits.append(exploited_q_table)\n",
    "            epislon -= self.decrease_factor #hyperparameter\n",
    "            \n",
    "        return all_rewards\n",
    "    \n",
    "    def run_episode(self,epislon):\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        state = self.continous_to_discrete(state)\n",
    "        \n",
    "        exploited_q_table = 0\n",
    "        \n",
    "        while not done:\n",
    "            random_number = np.random.random()\n",
    "            if random_number < epislon:\n",
    "                #explore\n",
    "                action = np.random.choice(self.num_of_actions)\n",
    "            else:\n",
    "                #exploit\n",
    "                action = self.get_action(state)\n",
    "                exploited_q_table+=1\n",
    "                \n",
    "            new_state, reward, done, i = self.env.step(action=action) # Q-learning\n",
    "            new_state = self.continous_to_discrete(new_state)\n",
    "            \n",
    "            #update q table\n",
    "            self.update_q_table(state,new_state,action,reward)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            state = new_state\n",
    "        return (rewards, exploited_q_table)\n",
    "    \n",
    "    def update_q_table(self,state,new_state,action,reward):\n",
    "        #target\n",
    "        #max of a' given the \n",
    "        max_a_prime = np.max(self.q_table.get_actions(new_state))\n",
    "        target = reward + (self.discount*max_a_prime)\n",
    "        \n",
    "        #compute difference\n",
    "        action_value = self.q_table.get_action_value(state,action)\n",
    "        difference = target - action_value\n",
    "        \n",
    "        #take a small step in the delta direction\n",
    "        new_q = action_value + (self.learning_rate * difference)\n",
    "        \n",
    "        self.q_table.set_action_value(state,action,new_q)\n",
    "        \n",
    "    \n",
    "    def get_action(self,state):\n",
    "        #exploit the q table\n",
    "        actions = self.q_table.get_actions(state)\n",
    "        action = np.argmax(self.q_table.get_actions(state))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter dump\n",
      "----\n",
      "Number Of Episodes = 2000\n",
      "Print out every 40 episodes\n",
      "Learning Rate = 0.25\n",
      "Discount = 0.99\n",
      "----\n",
      "Initial Epsilon = 1\n",
      "Epsilon Decrease Factor = 0.0004\n",
      "----\n",
      "Number of Bins to Discretise State = 20\n",
      "----\n",
      "Episode number: 40. Total reward in episode: -2.0787121598145073. Episode executed with epsilon = 0.9844000000000017\n",
      "Average total reward in last 40 episodes: -1.6005948167025672\n",
      "Average number of times we exploited q table in last 40 episodes: 0.4358974358974359\n",
      "-----\n",
      "Episode number: 80. Total reward in episode: -1.8484907226700567. Episode executed with epsilon = 0.9684000000000035\n",
      "Average total reward in last 40 episodes: -1.6368228308587138\n",
      "Average number of times we exploited q table in last 40 episodes: 1.25\n",
      "-----\n",
      "Episode number: 120. Total reward in episode: -1.625091768120587. Episode executed with epsilon = 0.9524000000000052\n",
      "Average total reward in last 40 episodes: -1.714146345759815\n",
      "Average number of times we exploited q table in last 40 episodes: 2.15\n",
      "-----\n",
      "Episode number: 160. Total reward in episode: -1.6720150780285241. Episode executed with epsilon = 0.936400000000007\n",
      "Average total reward in last 40 episodes: -1.643044023426743\n",
      "Average number of times we exploited q table in last 40 episodes: 3.1\n",
      "-----\n",
      "Episode number: 200. Total reward in episode: -1.7430739873441394. Episode executed with epsilon = 0.9204000000000088\n",
      "Average total reward in last 40 episodes: -1.6787367780643017\n",
      "Average number of times we exploited q table in last 40 episodes: 3.55\n",
      "-----\n",
      "Episode number: 240. Total reward in episode: -2.038154593839459. Episode executed with epsilon = 0.9044000000000105\n",
      "Average total reward in last 40 episodes: -1.633307142368114\n",
      "Average number of times we exploited q table in last 40 episodes: 4.4\n",
      "-----\n",
      "Episode number: 280. Total reward in episode: -1.9211218736407294. Episode executed with epsilon = 0.8884000000000123\n",
      "Average total reward in last 40 episodes: -1.7076036000547723\n",
      "Average number of times we exploited q table in last 40 episodes: 5.375\n",
      "-----\n",
      "Episode number: 320. Total reward in episode: -2.0657431662695873. Episode executed with epsilon = 0.872400000000014\n",
      "Average total reward in last 40 episodes: -1.7059798128947619\n",
      "Average number of times we exploited q table in last 40 episodes: 5.775\n",
      "-----\n",
      "Episode number: 360. Total reward in episode: -1.8439851107930487. Episode executed with epsilon = 0.8564000000000158\n",
      "Average total reward in last 40 episodes: -1.6572170588581536\n",
      "Average number of times we exploited q table in last 40 episodes: 6.375\n",
      "-----\n",
      "Episode number: 400. Total reward in episode: -1.6119523220150547. Episode executed with epsilon = 0.8404000000000176\n",
      "Average total reward in last 40 episodes: -1.7183188838759378\n",
      "Average number of times we exploited q table in last 40 episodes: 8.05\n",
      "-----\n",
      "Episode number: 440. Total reward in episode: -1.6287852741717563. Episode executed with epsilon = 0.8244000000000193\n",
      "Average total reward in last 40 episodes: -1.5804567966869216\n",
      "Average number of times we exploited q table in last 40 episodes: 9.05\n",
      "-----\n",
      "Episode number: 480. Total reward in episode: -1.7453379151095143. Episode executed with epsilon = 0.8084000000000211\n",
      "Average total reward in last 40 episodes: -1.6262193800667801\n",
      "Average number of times we exploited q table in last 40 episodes: 9.45\n",
      "-----\n",
      "Episode number: 520. Total reward in episode: -1.780355331697039. Episode executed with epsilon = 0.7924000000000229\n",
      "Average total reward in last 40 episodes: -1.6490972158690493\n",
      "Average number of times we exploited q table in last 40 episodes: 10.725\n",
      "-----\n",
      "Episode number: 560. Total reward in episode: -1.945031828670969. Episode executed with epsilon = 0.7764000000000246\n",
      "Average total reward in last 40 episodes: -1.5982805582760218\n",
      "Average number of times we exploited q table in last 40 episodes: 11.875\n",
      "-----\n",
      "Episode number: 600. Total reward in episode: -1.913321592544035. Episode executed with epsilon = 0.7604000000000264\n",
      "Average total reward in last 40 episodes: -1.6722253720262916\n",
      "Average number of times we exploited q table in last 40 episodes: 12.175\n",
      "-----\n",
      "Episode number: 640. Total reward in episode: -1.4772868134973236. Episode executed with epsilon = 0.7444000000000282\n",
      "Average total reward in last 40 episodes: -1.6341412865628846\n",
      "Average number of times we exploited q table in last 40 episodes: 13.85\n",
      "-----\n",
      "Episode number: 680. Total reward in episode: -1.4171416820177039. Episode executed with epsilon = 0.7284000000000299\n",
      "Average total reward in last 40 episodes: -1.630268931322714\n",
      "Average number of times we exploited q table in last 40 episodes: 13.35\n",
      "-----\n",
      "Episode number: 720. Total reward in episode: -1.7126336715914787. Episode executed with epsilon = 0.7124000000000317\n",
      "Average total reward in last 40 episodes: -1.602850210912144\n",
      "Average number of times we exploited q table in last 40 episodes: 14.225\n",
      "-----\n",
      "Episode number: 760. Total reward in episode: -1.5153910106314636. Episode executed with epsilon = 0.6964000000000334\n",
      "Average total reward in last 40 episodes: -1.6155770387549766\n",
      "Average number of times we exploited q table in last 40 episodes: 15.075\n",
      "-----\n",
      "Episode number: 800. Total reward in episode: -1.351756875716596. Episode executed with epsilon = 0.6804000000000352\n",
      "Average total reward in last 40 episodes: -1.630281392376574\n",
      "Average number of times we exploited q table in last 40 episodes: 15.65\n",
      "-----\n",
      "Episode number: 840. Total reward in episode: -1.7454535606604293. Episode executed with epsilon = 0.664400000000037\n",
      "Average total reward in last 40 episodes: -1.6706355859149753\n",
      "Average number of times we exploited q table in last 40 episodes: 17.55\n",
      "-----\n",
      "Episode number: 880. Total reward in episode: -1.7744765909152527. Episode executed with epsilon = 0.6484000000000387\n",
      "Average total reward in last 40 episodes: -1.6448101290883197\n",
      "Average number of times we exploited q table in last 40 episodes: 17.425\n",
      "-----\n",
      "Episode number: 920. Total reward in episode: -1.7258515544887385. Episode executed with epsilon = 0.6324000000000405\n",
      "Average total reward in last 40 episodes: -1.6659116587755576\n",
      "Average number of times we exploited q table in last 40 episodes: 19.55\n",
      "-----\n",
      "Episode number: 960. Total reward in episode: -1.4661085487016066. Episode executed with epsilon = 0.6164000000000422\n",
      "Average total reward in last 40 episodes: -1.6021265065907708\n",
      "Average number of times we exploited q table in last 40 episodes: 18.8\n",
      "-----\n",
      "Episode number: 1000. Total reward in episode: -1.8256915158258813. Episode executed with epsilon = 0.600400000000044\n",
      "Average total reward in last 40 episodes: -1.627582300611472\n",
      "Average number of times we exploited q table in last 40 episodes: 20.9\n",
      "-----\n",
      "Episode number: 1040. Total reward in episode: -1.7699553582639727. Episode executed with epsilon = 0.5844000000000458\n",
      "Average total reward in last 40 episodes: -1.614693428051691\n",
      "Average number of times we exploited q table in last 40 episodes: 21.875\n",
      "-----\n",
      "Episode number: 1080. Total reward in episode: -1.4034792278782362. Episode executed with epsilon = 0.5684000000000475\n",
      "Average total reward in last 40 episodes: -1.5972800862372896\n",
      "Average number of times we exploited q table in last 40 episodes: 21.975\n",
      "-----\n",
      "Episode number: 1120. Total reward in episode: -1.4069654973807253. Episode executed with epsilon = 0.5524000000000493\n",
      "Average total reward in last 40 episodes: -1.6251906299390655\n",
      "Average number of times we exploited q table in last 40 episodes: 23.35\n",
      "-----\n",
      "Episode number: 1160. Total reward in episode: -1.6640456512445085. Episode executed with epsilon = 0.5364000000000511\n",
      "Average total reward in last 40 episodes: -1.5683811045594869\n",
      "Average number of times we exploited q table in last 40 episodes: 23.25\n",
      "-----\n",
      "Episode number: 1200. Total reward in episode: -1.4484530077369733. Episode executed with epsilon = 0.5204000000000528\n",
      "Average total reward in last 40 episodes: -1.6047866823031343\n",
      "Average number of times we exploited q table in last 40 episodes: 23.55\n",
      "-----\n",
      "Episode number: 1240. Total reward in episode: -1.4691046337904017. Episode executed with epsilon = 0.5044000000000546\n",
      "Average total reward in last 40 episodes: -1.5661593053042207\n",
      "Average number of times we exploited q table in last 40 episodes: 25.35\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number: 1280. Total reward in episode: -1.826881864986881. Episode executed with epsilon = 0.48840000000005473\n",
      "Average total reward in last 40 episodes: -1.6288770970265027\n",
      "Average number of times we exploited q table in last 40 episodes: 25.3\n",
      "-----\n",
      "Episode number: 1320. Total reward in episode: -1.7974512406754635. Episode executed with epsilon = 0.4724000000000543\n",
      "Average total reward in last 40 episodes: -1.5803107731171218\n",
      "Average number of times we exploited q table in last 40 episodes: 27.625\n",
      "-----\n",
      "Episode number: 1360. Total reward in episode: -1.8257774564501008. Episode executed with epsilon = 0.4564000000000538\n",
      "Average total reward in last 40 episodes: -1.5734463367990428\n",
      "Average number of times we exploited q table in last 40 episodes: 28.25\n",
      "-----\n",
      "Episode number: 1400. Total reward in episode: -1.8668698225426226. Episode executed with epsilon = 0.44040000000005336\n",
      "Average total reward in last 40 episodes: -1.5657419735834583\n",
      "Average number of times we exploited q table in last 40 episodes: 29.5\n",
      "-----\n",
      "Episode number: 1440. Total reward in episode: -1.4636877938628559. Episode executed with epsilon = 0.4244000000000529\n",
      "Average total reward in last 40 episodes: -1.533256922175652\n",
      "Average number of times we exploited q table in last 40 episodes: 29.775\n",
      "-----\n",
      "Episode number: 1480. Total reward in episode: -1.5772399889057158. Episode executed with epsilon = 0.40840000000005244\n",
      "Average total reward in last 40 episodes: -1.6005631346089\n",
      "Average number of times we exploited q table in last 40 episodes: 31.325\n",
      "-----\n",
      "Episode number: 1520. Total reward in episode: -1.6910296869526606. Episode executed with epsilon = 0.392400000000052\n",
      "Average total reward in last 40 episodes: -1.5456338732315016\n",
      "Average number of times we exploited q table in last 40 episodes: 30.875\n",
      "-----\n",
      "Episode number: 1560. Total reward in episode: -1.3628043832533523. Episode executed with epsilon = 0.3764000000000515\n",
      "Average total reward in last 40 episodes: -1.592667209560243\n",
      "Average number of times we exploited q table in last 40 episodes: 30.7\n",
      "-----\n",
      "Episode number: 1600. Total reward in episode: -1.4605960019091855. Episode executed with epsilon = 0.36040000000005107\n",
      "Average total reward in last 40 episodes: -1.5528311656419258\n",
      "Average number of times we exploited q table in last 40 episodes: 33.625\n",
      "-----\n",
      "Episode number: 1640. Total reward in episode: -1.6111034295626583. Episode executed with epsilon = 0.3444000000000506\n",
      "Average total reward in last 40 episodes: -1.5960081633423535\n",
      "Average number of times we exploited q table in last 40 episodes: 32.8\n",
      "-----\n",
      "Episode number: 1680. Total reward in episode: -1.6962795721483845. Episode executed with epsilon = 0.32840000000005015\n",
      "Average total reward in last 40 episodes: -1.5488517529148647\n",
      "Average number of times we exploited q table in last 40 episodes: 34.45\n",
      "-----\n",
      "Episode number: 1720. Total reward in episode: -1.4894893150359285. Episode executed with epsilon = 0.3124000000000497\n",
      "Average total reward in last 40 episodes: -1.510551362132866\n",
      "Average number of times we exploited q table in last 40 episodes: 35.15\n",
      "-----\n",
      "Episode number: 1760. Total reward in episode: -1.8764926488650726. Episode executed with epsilon = 0.29640000000004924\n",
      "Average total reward in last 40 episodes: -1.5523354316301359\n",
      "Average number of times we exploited q table in last 40 episodes: 36.1\n",
      "-----\n",
      "Episode number: 1800. Total reward in episode: -1.4641983434728587. Episode executed with epsilon = 0.2804000000000488\n",
      "Average total reward in last 40 episodes: -1.5108320589341846\n",
      "Average number of times we exploited q table in last 40 episodes: 36.525\n",
      "-----\n",
      "Episode number: 1840. Total reward in episode: -1.7620943504165933. Episode executed with epsilon = 0.2644000000000483\n",
      "Average total reward in last 40 episodes: -1.5188715194898108\n",
      "Average number of times we exploited q table in last 40 episodes: 38.025\n",
      "-----\n",
      "Episode number: 1880. Total reward in episode: -1.353484486107406. Episode executed with epsilon = 0.24840000000004786\n",
      "Average total reward in last 40 episodes: -1.5115529323144299\n",
      "Average number of times we exploited q table in last 40 episodes: 39.225\n",
      "-----\n",
      "Episode number: 1920. Total reward in episode: -1.5744653360964433. Episode executed with epsilon = 0.2324000000000474\n",
      "Average total reward in last 40 episodes: -1.5415394714828228\n",
      "Average number of times we exploited q table in last 40 episodes: 39.475\n",
      "-----\n",
      "Episode number: 1960. Total reward in episode: -1.4913168210796628. Episode executed with epsilon = 0.21640000000004694\n",
      "Average total reward in last 40 episodes: -1.5135783803241356\n",
      "Average number of times we exploited q table in last 40 episodes: 40.5\n",
      "-----\n",
      "Episode number: 2000. Total reward in episode: -1.4436827592970023. Episode executed with epsilon = 0.20040000000004649\n",
      "Average total reward in last 40 episodes: -1.4656057312474995\n",
      "Average number of times we exploited q table in last 40 episodes: 40.55\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "agent = QLearningAgent(env)\n",
    "rewards = agent.run_all_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(agent, rewards):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    axes[1].plot(rewards);\n",
    "    axes[1].set_xlabel('episode')\n",
    "    axes[1].set_ylabel('total reward r(t)')\n",
    "    \n",
    "plot(agent, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
