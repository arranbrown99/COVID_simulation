{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(0.0, inf, (4,), float64)\n",
      "[[6.69911121e-02 9.01684654e-01 3.01429427e+00 1.28687232e+00]\n",
      " [7.50381696e-01 9.89018225e-02 2.00181818e-01 1.36400919e+00]\n",
      " [6.52932679e-01 8.57530100e-02 6.15337927e-01 6.82255418e-01]\n",
      " ...\n",
      " [1.37099713e+00 9.80472801e-02 1.88245692e-01 1.36117011e-01]\n",
      " [1.53520688e+00 9.35774558e-01 1.18393981e+00 2.29107216e-04]\n",
      " [4.54582924e-01 3.71918634e-01 5.29771570e-01 7.14922373e-01]]\n"
     ]
    }
   ],
   "source": [
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(observation_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import basic tools for defining the function and doing the gradient-based learning\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "#from sklearn.preprocessing import PolynomialFeatures # you can try with polynomial basis if you want (It is difficult!)\n",
    "from sklearn.linear_model import SGDRegressor # this defines the SGD function\n",
    "from sklearn.kernel_approximation import RBFSampler # this is the RBF function transformation method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearAprxAgent:\n",
    "    def create_policy(self,func_approximator, epsilon):\n",
    "        # from lab 8\n",
    "        def policy_fn(state):\n",
    "            \"\"\"\n",
    "            \n",
    "            Input:\n",
    "                state: a 2D array with the position and velocity\n",
    "            Output:\n",
    "                A,q_values: \n",
    "            \"\"\"\n",
    "            action_index = np.ones(self.num_of_actions, dtype=float) * epsilon / self.num_of_actions\n",
    "            \n",
    "            #transform to the same shape as the model was trained on\n",
    "            state_transformed = self.feature_transformer.transform([state])\n",
    "            q_values = self.func_approximator.predict(state_transformed)\n",
    "           \n",
    "            best_action = np.argmax(q_values)\n",
    "            action_index[best_action] += (1.0 - epsilon)\n",
    "            \n",
    "            \n",
    "            return action_index,q_values  # return the potentially stochastic policy (which is due to the exploration)\n",
    "\n",
    "        return policy_fn # return a handle to the function so we can call it in the future\n",
    "        \n",
    "    def __init__(self,env):\n",
    "        \n",
    "        #RBF Hyper parameters\n",
    "        SGD_learning_rate = \"optimal\" #‘constant’, ‘optimal’, ‘invscaling’, ‘adaptive’\n",
    "        tol = 1e-5 #The stopping criterion\n",
    "        SGD_max_iter = 1e4\n",
    "        \n",
    "        \n",
    "        self.func_approximator =  SGDRegressor(learning_rate=SGD_learning_rate, tol=tol, max_iter=SGD_max_iter, loss='huber')\n",
    "        self.feature_transformer =  sklearn.pipeline.FeatureUnion([\n",
    "                (\"rbf1\", RBFSampler(gamma=12.8, n_components=50)),\n",
    "                (\"rbf2\", RBFSampler(gamma=6.4, n_components=50)),\n",
    "                (\"rbf3\", RBFSampler(gamma=3.2, n_components=50)),\n",
    "                (\"rbf4\", RBFSampler(gamma=1.6, n_components=50)),\n",
    "                (\"rbf5\", RBFSampler(gamma=0.8, n_components=50)),\n",
    "                (\"rbf6\", RBFSampler(gamma=0.4, n_components=50)),\n",
    "                (\"rbf7\", RBFSampler(gamma=0.2, n_components=50)),\n",
    "                (\"rbf8\", RBFSampler(gamma=0.1, n_components=50))\n",
    "                ])\n",
    "        \n",
    "        \n",
    "        self.num_of_actions = env.action_space.n\n",
    "        self.env = env\n",
    "        self.policy = self.create_policy(self.func_approximator,1)\n",
    "        \n",
    "        self.episodes = 200\n",
    "        self.print_out_every_x_episodes = int(self.episodes/50)\n",
    "        self.times_exploited = 0\n",
    "        \n",
    "        # hyper parameters for epsilon explore\n",
    "        self.initial_epsilon = 1 # initial\n",
    "        self.decrease_factor = (1/self.episodes)/1.25 # epsilon\n",
    "        print(\"Decrease Factor: \" + str(self.decrease_factor))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def run_all_episodes(self):\n",
    "        all_rewards = []\n",
    "        epsilon = self.initial_epsilon # at the start only explore\n",
    "        \n",
    "        \n",
    "        power = 1\n",
    "        for episode in range(1, self.episodes + 1):\n",
    "            states,rewards = self.run_episode(epsilon)\n",
    "            total_reward = np.sum(rewards)\n",
    "\n",
    "            \n",
    "            \n",
    "            if episode % self.print_out_every_x_episodes == 0:\n",
    "                print(\"Episode number: \" + str(episode) + \". Total reward in episode: \" + str(total_reward) + \". Episode executed with epsilon = \" + str(epsilon))\n",
    "                print(\"Average total reward in last \" + str(self.print_out_every_x_episodes) + \" episodes: \" + str(np.mean(all_rewards[-self.print_out_every_x_episodes:])))\n",
    "                print(\"Times exploited the last episode \" + str(self.times_exploited))\n",
    "                print(\"-----\")\n",
    "            self.times_exploited = 0\n",
    "            all_rewards.append(total_reward)\n",
    "            epsilon = self.decrease_epsilon(epsilon, power)\n",
    "            power += 0.10\n",
    "        #returns all rewards and the last episodes state\n",
    "        return states,all_rewards\n",
    "    \n",
    "    \n",
    "    #exponential decrease in epsilon\n",
    "    def decrease_epsilon(self, epsilon, power):\n",
    "        decrease = 0.005\n",
    "        return epsilon * ((1 - decrease) ** power)\n",
    "    \n",
    "    def run_episode(self,epsilon):\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        done = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        states.append(state)\n",
    "        \n",
    "      \n",
    "        \n",
    "        while not done:\n",
    "            random_number = np.random.random()\n",
    "            if random_number < epsilon:\n",
    "                #explore\n",
    "                action = np.random.choice(self.num_of_actions)\n",
    "                \n",
    "            else:\n",
    "                #exploit\n",
    "                action = self.get_action(state)\n",
    "                self.times_exploited += 1\n",
    "              \n",
    "            \n",
    "            new_state, reward, done, i = self.env.step(action=action)\n",
    "            \n",
    "            states.append(new_state)\n",
    "            actions.append(action)    \n",
    "            rewards.append(reward)\n",
    "            \n",
    "            #update policy function\n",
    "            self.update(states[1:],rewards, epsilon)\n",
    "        \n",
    "            \n",
    "            state = new_state\n",
    "        return states,rewards\n",
    "        \n",
    "    def update(self,states,rewards, epsilon):\n",
    "        \n",
    "        #update the linear function\n",
    "        self.feature_transformer.fit(states)\n",
    "        states_transformed = self.feature_transformer.transform(states)\n",
    "        \n",
    "        \n",
    "        self.func_approximator.fit(states_transformed,rewards)\n",
    "        self.policy = self.create_policy(self.func_approximator,epsilon)\n",
    "                                                       \n",
    "        \n",
    "    def get_action(self,state):\n",
    "        #linear function to get best action\n",
    "        actions,q_values = self.policy(state)\n",
    "        return np.argmax(actions)\n",
    "    \n",
    "    def get_action_text(self):\n",
    "        return action_text\n",
    "    \n",
    "    def get_env(self):\n",
    "        return env\n",
    "    \n",
    "    def get_chart_title(self):\n",
    "        return \"Action = \" + action_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease Factor: 0.004\n",
      "Episode number: 4. Total reward in episode: -2.206319123429255. Episode executed with epsilon = 0.98359466951464\n",
      "Average total reward in last 4 episodes: -1.641818549268202\n",
      "Times exploited the last episode 0\n",
      "-----\n",
      "Episode number: 8. Total reward in episode: -1.7328968584959636. Episode executed with epsilon = 0.9554105547748558\n",
      "Average total reward in last 4 episodes: -1.768767279832113\n",
      "Times exploited the last episode 0\n",
      "-----\n",
      "Episode number: 12. Total reward in episode: -1.8620227215240666. Episode executed with epsilon = 0.920620904771527\n",
      "Average total reward in last 4 episodes: -1.5353266916708717\n",
      "Times exploited the last episode 3\n",
      "-----\n",
      "Episode number: 16. Total reward in episode: -1.596295537695248. Episode executed with epsilon = 0.8800119284892101\n",
      "Average total reward in last 4 episodes: -1.7555289121040072\n",
      "Times exploited the last episode 3\n",
      "-----\n",
      "Episode number: 20. Total reward in episode: -1.26684016638791. Episode executed with epsilon = 0.8344747784951227\n",
      "Average total reward in last 4 episodes: -1.7283260510578695\n",
      "Times exploited the last episode 7\n",
      "-----\n",
      "Episode number: 24. Total reward in episode: -1.6873662339304272. Episode executed with epsilon = 0.7849731464905104\n",
      "Average total reward in last 4 episodes: -1.6384828035899355\n",
      "Times exploited the last episode 8\n",
      "-----\n",
      "Episode number: 28. Total reward in episode: -1.6813615878368653. Episode executed with epsilon = 0.7325095891095212\n",
      "Average total reward in last 4 episodes: -1.637888243205633\n",
      "Times exploited the last episode 5\n",
      "-----\n",
      "Episode number: 32. Total reward in episode: -1.8426507295337258. Episode executed with epsilon = 0.6780922140785683\n",
      "Average total reward in last 4 episodes: -1.6570790753498605\n",
      "Times exploited the last episode 13\n",
      "-----\n",
      "Episode number: 36. Total reward in episode: -1.7824771599064577. Episode executed with epsilon = 0.622703247251223\n",
      "Average total reward in last 4 episodes: -1.8798309897210286\n",
      "Times exploited the last episode 24\n",
      "-----\n",
      "Episode number: 40. Total reward in episode: -1.9178991588887193. Episode executed with epsilon = 0.567270804320246\n",
      "Average total reward in last 4 episodes: -1.7153630078545843\n",
      "Times exploited the last episode 21\n",
      "-----\n",
      "Episode number: 44. Total reward in episode: -1.9222994551552408. Episode executed with epsilon = 0.5126449262191629\n",
      "Average total reward in last 4 episodes: -1.8092850607794992\n",
      "Times exploited the last episode 30\n",
      "-----\n",
      "Episode number: 48. Total reward in episode: -1.6859497417315041. Episode executed with epsilon = 0.45957862733936494\n",
      "Average total reward in last 4 episodes: -1.7733737903903126\n",
      "Times exploited the last episode 21\n",
      "-----\n",
      "Episode number: 52. Total reward in episode: -1.9217131662491314. Episode executed with epsilon = 0.4087143754208219\n",
      "Average total reward in last 4 episodes: -1.7307160070250556\n",
      "Times exploited the last episode 31\n",
      "-----\n",
      "Episode number: 56. Total reward in episode: -1.8089032242270642. Episode executed with epsilon = 0.3605760955804251\n",
      "Average total reward in last 4 episodes: -1.6878266032978986\n",
      "Times exploited the last episode 29\n",
      "-----\n",
      "Episode number: 60. Total reward in episode: -1.8654392728629414. Episode executed with epsilon = 0.31556649023902916\n",
      "Average total reward in last 4 episodes: -1.7223652127922966\n",
      "Times exploited the last episode 35\n",
      "-----\n",
      "Episode number: 64. Total reward in episode: -1.6960727644864795. Episode executed with epsilon = 0.27396920949304415\n",
      "Average total reward in last 4 episodes: -1.7544246627297746\n",
      "Times exploited the last episode 33\n",
      "-----\n",
      "Episode number: 68. Total reward in episode: -1.8686284540011675. Episode executed with epsilon = 0.23595520545575277\n",
      "Average total reward in last 4 episodes: -1.7169048549100188\n",
      "Times exploited the last episode 41\n",
      "-----\n",
      "Episode number: 72. Total reward in episode: -1.7527711178064205. Episode executed with epsilon = 0.20159246638153105\n",
      "Average total reward in last 4 episodes: -1.8294909700577247\n",
      "Times exploited the last episode 42\n",
      "-----\n",
      "Episode number: 76. Total reward in episode: -1.752809570764211. Episode executed with epsilon = 0.17085825355648257\n",
      "Average total reward in last 4 episodes: -1.7850763676071408\n",
      "Times exploited the last episode 43\n",
      "-----\n",
      "Episode number: 80. Total reward in episode: -1.7468063002965613. Episode executed with epsilon = 0.1436529525640109\n",
      "Average total reward in last 4 episodes: -1.8106890250080312\n",
      "Times exploited the last episode 39\n",
      "-----\n",
      "Episode number: 84. Total reward in episode: -1.7009044637317903. Episode executed with epsilon = 0.11981469310041844\n",
      "Average total reward in last 4 episodes: -1.7476291824755414\n",
      "Times exploited the last episode 49\n",
      "-----\n",
      "Episode number: 88. Total reward in episode: -1.7126029600572987. Episode executed with epsilon = 0.0991339775819363\n",
      "Average total reward in last 4 episodes: -1.6969825722318224\n",
      "Times exploited the last episode 50\n",
      "-----\n",
      "Episode number: 92. Total reward in episode: -1.7126029710132626. Episode executed with epsilon = 0.08136767623533221\n",
      "Average total reward in last 4 episodes: -1.7090283270524993\n",
      "Times exploited the last episode 51\n",
      "-----\n",
      "Episode number: 96. Total reward in episode: -1.736835114853434. Episode executed with epsilon = 0.06625188263090968\n",
      "Average total reward in last 4 episodes: -1.7303444696023609\n",
      "Times exploited the last episode 47\n",
      "-----\n",
      "Episode number: 100. Total reward in episode: -1.7487619594732124. Episode executed with epsilon = 0.05351326675286681\n",
      "Average total reward in last 4 episodes: -1.6920750515618517\n",
      "Times exploited the last episode 47\n",
      "-----\n",
      "Episode number: 104. Total reward in episode: -1.7171828088498873. Episode executed with epsilon = 0.04287870223442939\n",
      "Average total reward in last 4 episodes: -1.7167940569295688\n",
      "Times exploited the last episode 50\n",
      "-----\n",
      "Episode number: 108. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.03408307189731432\n",
      "Average total reward in last 4 episodes: -1.706360856571094\n",
      "Times exploited the last episode 51\n",
      "-----\n",
      "Episode number: 112. Total reward in episode: -1.7008996510460055. Episode executed with epsilon = 0.02687526517282434\n",
      "Average total reward in last 4 episodes: -1.7120894378903713\n",
      "Times exploited the last episode 50\n",
      "-----\n",
      "Episode number: 116. Total reward in episode: -1.7126029742579916. Episode executed with epsilon = 0.02102246870206909\n",
      "Average total reward in last 4 episodes: -1.7010628052342485\n",
      "Times exploited the last episode 51\n",
      "-----\n",
      "Episode number: 120. Total reward in episode: -1.692403742374081. Episode executed with epsilon = 0.016312916016255435\n",
      "Average total reward in last 4 episodes: -1.689040802025362\n",
      "Times exploited the last episode 50\n",
      "-----\n",
      "Episode number: 124. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.012557304202962876\n",
      "Average total reward in last 4 episodes: -1.6918450207347155\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 128. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.00958910688153327\n",
      "Average total reward in last 4 episodes: -1.6816346658740122\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 132. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.007264016678532817\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 136. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.005458740324866718\n",
      "Average total reward in last 4 episodes: -1.684210650530507\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 140. Total reward in episode: -1.712602945012265. Episode executed with epsilon = 0.0040693492716173145\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 51\n",
      "-----\n",
      "Episode number: 144. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0030093619657222503\n",
      "Average total reward in last 4 episodes: -1.6890407947139305\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 148. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0022077038644878247\n",
      "Average total reward in last 4 episodes: -1.6858251072691606\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 152. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0016066605759151868\n",
      "Average total reward in last 4 episodes: -1.6833617732052928\n",
      "Times exploited the last episode 52\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number: 156. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0011599102482570333\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 160. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0008306949365847114\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 164. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0005901680170977277\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 168. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.0004159361869823121\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 172. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.00029080018028361813\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 176. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.00020168777080309993\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 180. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 0.00013876546313868303\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 184. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 9.471093898510693e-05\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 188. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 6.412624712440617e-05\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 192. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 4.307134517377923e-05\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 196. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 2.869841925825482e-05\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n",
      "Episode number: 200. Total reward in episode: -1.6811867446144855. Episode executed with epsilon = 1.8969000393982113e-05\n",
      "Average total reward in last 4 episodes: -1.6811867446144855\n",
      "Times exploited the last episode 52\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "agent = LinearAprxAgent(env)\n",
    "states,all_rewards = agent.run_all_episodes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "def plot(state,agent, rewards):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    labels = ['s[0]: susceptibles', 's[1]: infectious', 's[2]: quarantined', 's[3]: recovereds']\n",
    "    states = np.array(state)\n",
    "    for i in range(4):\n",
    "        axes[0].plot(states[:,i], label=labels[i]);\n",
    "    axes[0].set_xlabel('weeks since start of epidemic')\n",
    "    axes[0].set_ylabel('State s(t)')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(rewards);\n",
    "    axes[1].set_xlabel('episode number')\n",
    "    axes[1].set_ylabel('total reward r(t)')\n",
    "    \n",
    "plot(states,agent, all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
