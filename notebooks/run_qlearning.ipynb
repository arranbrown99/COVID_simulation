{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from helper_methods import run, plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "\n",
    "    def __init__(self, env,number_bins):\n",
    "        self.num_of_actions = env.action_space.n\n",
    "        #self.state_space =\n",
    "        self.env = env\n",
    "        self.number_bins = number_bins\n",
    "        \n",
    "        self.q_table = {}\n",
    "           \n",
    "        \n",
    "        # hyper parameters\n",
    "        self.discount = 0.99 #gamma\n",
    "        self.learning_rate = 0.2 #step size, alpha\n",
    "        self.episodes = 200\n",
    "        self.decrease_factor = 0.01 #epislon\n",
    "        \n",
    "        \n",
    "    def continous_to_discrete(self,continous,highest=600000000,lowest=0):\n",
    "        bins = np.linspace(lowest,highest,num=self.number_bins)\n",
    "        discrete = np.digitize(continous,bins)\n",
    "        return discrete\n",
    "    \n",
    "    def run_all_episodes(self):\n",
    "        all_rewards = []\n",
    "        epislon = 1 #at the start only explore\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            rewards = self.run_episode(epislon)\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "            epislon -= self.decrease_factor #hyperparameter\n",
    "            \n",
    "        return all_rewards\n",
    "    \n",
    "    def run_episode(self,epislon):\n",
    "        states = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        \n",
    "        state = self.env.reset()\n",
    "        state = self.continous_to_discrete(state)\n",
    "        self.q_table[tuple(state)] = [0,0,0,0]\n",
    "        states.append(state)\n",
    "        \n",
    "        while not done:\n",
    "            random_number = np.random.randint(0,1)\n",
    "            if random_number < epislon:\n",
    "                #explore\n",
    "                action = np.random.choice(self.num_of_actions)\n",
    "            else:\n",
    "                #exploit\n",
    "                action = self.get_action(state)\n",
    "                \n",
    "            new_state, reward, done, i = self.env.step(action=action) # Q-learning\n",
    "            new_state = self.continous_to_discrete(new_state)\n",
    "            self.q_table[tuple(new_state)] = [0,0,0,0]\n",
    "            \n",
    "            #update q table\n",
    "            self.update_q_table(state,new_state,action,reward)\n",
    "            \n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            state = new_state\n",
    "        return rewards\n",
    "    \n",
    "    def update_q_table(self,state,new_state,action,reward):\n",
    "        #target\n",
    "        #max of a' given the \n",
    "        max_a_prime = np.max(self.value_from_q(new_state))\n",
    "        target = reward + (self.discount*max_a_prime)\n",
    "        \n",
    "        #compute difference\n",
    "        difference = target - self.value_from_q(state)[action]\n",
    "        \n",
    "        #take a small step in the delta direction\n",
    "        new_q = self.value_from_q(state)[action] + (self.learning_rate * difference)\n",
    "        if tuple(state) in self.q_table:\n",
    "            self.q_table[tuple(state)][action] = new_q\n",
    "        else:\n",
    "            self.q_table[tuple(state)] = [0,0,0,0]\n",
    "            self.q_table[tuple(state)][action] = new_q\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        #exploit the q table\n",
    "        if tuple(state) in self.q_table:\n",
    "            action = np.argmax(self.q_table[tuple(state)])\n",
    "        else:\n",
    "            self.q_table[tuple(state)] = [0,0,0,0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def value_from_q(self,state):\n",
    "        return self.q_table.get(tuple(state),[0,0,0,0])\n",
    "    \n",
    "            \n",
    "            \n",
    "    def get_action_text(self,action):\n",
    "        action_texts = [\"no intervention\", \"impose a full lockdown\", \"implement track & trace\", \"enforce social distancing and face masks\"]\n",
    "        return action_texts[action]\n",
    "    \n",
    "    def get_env(self):\n",
    "        return env\n",
    "    \n",
    "    def get_chart_title(self):\n",
    "        return \"Title here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "\n",
    "agent = QLearningAgent(env,60)\n",
    "rewards = agent.run_all_episodes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(agent, rewards):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    axes[1].plot(rewards);\n",
    "    axes[1].set_xlabel('episode')\n",
    "    axes[1].set_ylabel('total reward r(t)')\n",
    "    \n",
    "plot(agent, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
