{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from operator import attrgetter\n",
    "import random\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and backend for neural networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow import where as tf_where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NNFunctionApproximatorJointKeras():\n",
    "    \n",
    "    def __init__(self, learning_rate, input_layer_dim, output_layer_dim, nn_config, verbose=False):        \n",
    "        self.learning_rate = learning_rate \n",
    "        self.nn_config = nn_config      # determines the size of the hidden layer (if any)              \n",
    "        self.input_layer_dim = input_layer_dim     \n",
    "        self.output_layer_dim = output_layer_dim  \n",
    "        self.verbose=verbose # Print debug information        \n",
    "        self.n_layers = len(nn_config)\n",
    "        self.model = self._build_model()  \n",
    "                        \n",
    "    def _huber_loss(self,y_true, y_pred, clip_delta=1.0):\n",
    "        \"\"\"\n",
    "        Huber loss (for use in Keras), see https://en.wikipedia.org/wiki/Huber_loss\n",
    "        The huber loss tends to provide more robust learning in RL settings where there are \n",
    "        often \"outliers\" before the functions has converged.\n",
    "        \"\"\"\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "        return K.mean(tf_where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning \n",
    "        model = Sequential()\n",
    "        for ilayer in self.nn_config:\n",
    "            model.add(Dense(ilayer, input_dim=self.input_layer_dim, activation='relu'))        \n",
    "        model.add(Dense(self.output_layer_dim, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss, # define a special loss function\n",
    "                      optimizer=Adam(lr=self.learning_rate, clipnorm=10.)) # specify the optimiser, we clip the gradient of the norm which can make traning more robust\n",
    "        return model\n",
    "\n",
    "    def predict(self, s, a=None):              \n",
    "        if a==None:            \n",
    "            return self._predict_nn(s)\n",
    "        else:                        \n",
    "            return self._predict_nn(s)[a]\n",
    "        \n",
    "    def _predict_nn(self,state_hat):                          \n",
    "        \"\"\"\n",
    "        Predict the output of the neural netwwork (note: these can be vectors)\n",
    "        \"\"\"                \n",
    "        x = self.model.predict(state_hat)                                                    \n",
    "        return x\n",
    "  \n",
    "    def update(self, states, td_target):\n",
    "        self.model.fit(states, td_target, epochs=1, verbose=0) # take one gradient step usign Adam               \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'new_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Implement a replay buffer using the deque collection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, batch_size):\n",
    "        self.capacity = size\n",
    "        self.memory = deque(maxlen=size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def pop(self):\n",
    "        return self.memory.pop()\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def can_sample(self):\n",
    "        return len(self.memory) >= self.batch_size\n",
    "    \n",
    "    def extract_samples(self):\n",
    "        returned_batch_size = int(self.batch_size/2)\n",
    "        \n",
    "        transitions = self.sample()\n",
    "        best_transitions = sorted(transitions, key=attrgetter(\"reward\"), reverse=True)\n",
    "        transitions = best_transitions[:returned_batch_size]\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        train_rewards = np.array(batch.reward)\n",
    "        train_states = np.array(batch.state)\n",
    "        train_new_state = np.array(batch.new_state)\n",
    "        train_actions = np.array(batch.action)\n",
    "        return train_rewards, train_states, train_new_state, train_actions, returned_batch_size\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env, learning_rate=0.001):\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.d_states = env.observation_space.shape[0]\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def get_action(self, strategy, policy_network, state):\n",
    "        action = self.get_random_action()\n",
    "        exploit = False\n",
    "        if not self.is_learning():\n",
    "            action = self.predict_action_from_nn(policy_network, state) # exploit\n",
    "            exploit = True\n",
    "        else:\n",
    "            if not strategy.should_explore():\n",
    "                action = self.predict_action_from_nn(policy_network, state) # exploit\n",
    "                exploit = True\n",
    "        return (action, exploit)\n",
    "    \n",
    "    def get_num_actions(self):\n",
    "        return self.n_actions\n",
    "    \n",
    "    def get_num_states(self):\n",
    "        return self.d_states\n",
    "    \n",
    "    def get_random_action(self):\n",
    "        return np.random.choice(self.get_num_actions())\n",
    "    \n",
    "    def predict_action_from_nn(self, policy_network, state):\n",
    "        action = policy_network.predict(state)[0]\n",
    "        action = np.argmax(action)\n",
    "        return action\n",
    "    \n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.get_num_states()])\n",
    "    \n",
    "    def is_learning(self):\n",
    "        return self.learning_rate > 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Strategy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy():\n",
    "    \n",
    "    def __init__(self, epsilon, epsilon_decay):\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "    def episode_complete(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def should_explore(self):\n",
    "        return np.random.random() < self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Networks Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_networks(input_layer_dim, output_layer_dim, learning_rate=0.001, nn_config=[24,24]):\n",
    "    # Init the two networks\n",
    "    policy_network = NNFunctionApproximatorJointKeras(learning_rate, input_layer_dim, output_layer_dim, nn_config)\n",
    "    target_network = NNFunctionApproximatorJointKeras(learning_rate, input_layer_dim, output_layer_dim, nn_config)\n",
    "    target_network.model.set_weights(policy_network.model.get_weights())\n",
    "    return (policy_network, target_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(all_rewards, smoothed_rewards):\n",
    "    plt.figure(2, figsize=(12, 6))\n",
    "    plt.clf()\n",
    "    plt.title(\"Training\")\n",
    "    plt.xlabel(\"Epsiode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.plot(all_rewards, '--', alpha=0.5)\n",
    "    plt.plot(smoothed_rewards)\n",
    "    plt.legend([\"Rewards\", \"Rewards (Smoothed)\"])\n",
    "    plt.pause(0.0001)\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "\n",
    "def qlearning_nn(agent, policy_network, target_network, num_episodes, window_size, strategy=None, memory=None, ):\n",
    "\n",
    "    if agent.is_learning() and memory is None:\n",
    "        print(\"Agent is learning, function requires memory\")\n",
    "    if agent.is_learning() and strategy is None:\n",
    "        print(\"Agent is learning, function requires strategy\")\n",
    "    \n",
    "    discount_factor = 0.95\n",
    "    env = agent.env\n",
    "    \n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        rewards = []\n",
    "        exploits = 0\n",
    "        \n",
    "        state = agent.preprocess_state(env.reset())\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            action, exploit = agent.get_action(strategy, policy_network, state)\n",
    "            if exploit:\n",
    "                exploits+=1\n",
    "                \n",
    "\n",
    "            new_state, reward, done, i = env.step(action=action)\n",
    "            new_state = agent.preprocess_state(new_state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if agent.is_learning():\n",
    "                memory.push(state, action, new_state, reward)\n",
    "        \n",
    "            if done:\n",
    "                if agent.is_learning():\n",
    "                    if memory.can_sample():\n",
    "                        # Fetch a batch from the replay buffer and extract as numpy arrays \n",
    "                        train_rewards, train_states, train_new_state, train_actions, batch_size = memory.extract_samples()\n",
    "\n",
    "                        q_values_for_current_state = policy_network.predict(train_states.reshape(batch_size,agent.get_num_states())) # predict current values for the given states\n",
    "                        q_values_for_new_state     = target_network.predict(train_new_state.reshape(batch_size,agent.get_num_states()))                    \n",
    "                        q_values_for_current_state_tmp = train_rewards + discount_factor * np.amax(q_values_for_new_state,axis=1)                \n",
    "                        q_values_for_current_state[ (np.arange(batch_size), train_actions.reshape(batch_size,).astype(int))] = q_values_for_current_state_tmp                                                                              \n",
    "                        policy_network.update(train_states.reshape(batch_size,agent.get_num_states()), q_values_for_current_state) # Update the function approximator \n",
    "                \n",
    "                if episode % 100 == 0:\n",
    "                    target_network.model.set_weights(policy_network.model.get_weights())\n",
    "                total_reward = np.sum(rewards)\n",
    "                all_rewards.append(total_reward)\n",
    "                \n",
    "                ## taking moving average of rewards to smooth\n",
    "                smoothed_rewards = pd.Series(all_rewards).rolling(window_size, min_periods=window_size).mean()\n",
    "                this_smoothed_reward = smoothed_rewards.values[-1]\n",
    "                \n",
    "                        \n",
    "                if num_episodes == 1:\n",
    "                    print(\"Evaluation reward \" + str(total_reward))\n",
    "                else:\n",
    "                    print(\"Episode = \" + str(episode) + \". Num Exploits = \" + str(exploits) + \". Total Reward = \" + str(total_reward)\n",
    "                          + \". Moving Average Reward = \" + str(this_smoothed_reward))\n",
    "                    plot(all_rewards, smoothed_rewards)\n",
    "\n",
    "\n",
    "                \n",
    "            state = new_state\n",
    "            if strategy:\n",
    "                strategy.episode_complete()\n",
    "    \n",
    "    return all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4bf6344d3082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-8-2083a7f4b6fe>\u001b[0m in \u001b[0;36mqlearning_nn\u001b[0;34m(agent, policy_network, target_network, num_episodes, window_size, strategy, memory)\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0mtrain_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_new_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                         \u001b[0mq_values_for_current_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# predict current values for the given states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                         \u001b[0mq_values_for_new_state\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_new_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                         \u001b[0mq_values_for_current_state_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values_for_new_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-221b19d298d4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-221b19d298d4>\u001b[0m in \u001b[0;36m_predict_nn\u001b[0;34m(self, state_hat)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mPredict\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mneural\u001b[0m \u001b[0mnetwwork\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthese\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \"\"\"                \n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 397\u001b[0;31m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1700\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m           preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4100\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4103\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, name)\u001b[0m\n\u001b[1;32m   5087\u001b[0m         \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5088\u001b[0m         \u001b[0;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"deterministic\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5089\u001b[0;31m         deterministic, \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[1;32m   5090\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5091\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 2000\n",
    "epsilon_decay = 0.999985\n",
    "window_size = 100\n",
    "\n",
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "agent = Agent(env, learning_rate=0.001)\n",
    "memory = ReplayMemory(size=520, batch_size=208)  # only actually gets the best 104 samples from 208\n",
    "\n",
    "strategy = Strategy(epsilon=1, epsilon_decay=epsilon_decay)\n",
    "policy_network, target_network = init_networks(agent.get_num_states(), agent.get_num_actions(), agent.learning_rate)\n",
    "\n",
    "rewards = qlearning_nn(\n",
    "    agent=agent, \n",
    "    policy_network=policy_network, \n",
    "    target_network=target_network,\n",
    "    num_episodes=episodes,\n",
    "    window_size=window_size, \n",
    "    strategy=strategy,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = virl.Epidemic(stochastic=True, noisy=False)\n",
    "\n",
    "agent = Agent(env, learning_rate=0.0)\n",
    "rewards = qlearning_nn(\n",
    "    agent=agent,\n",
    "    policy_network=policy_network,\n",
    "    target_network=target_network,\n",
    "    num_episodes=1,\n",
    "    window_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_with(decay, episodes):\n",
    "    print(\"Epsilon with decay \" + str(decay) + \" and \" + str(episodes) + \" episodes\")\n",
    "    check_epsilon_after(decay, int(episodes/4))    \n",
    "    check_epsilon_after(decay, int(episodes/2))    \n",
    "    check_epsilon_after(decay, int((episodes*3)/4))    \n",
    "    check_epsilon_after(decay, int(episodes))\n",
    "    print(\"------\")\n",
    "\n",
    "def check_epsilon_after(decay, episodes):\n",
    "    value = (decay**52)**episodes\n",
    "    print(\"After \" + str(episodes) + \" episodes, epsilon will be \" + str(value))\n",
    "    \n",
    "# epsilon_with(0.99989, 300)\n",
    "# epsilon_with(0.999986, 3000)\n",
    "# epsilon_with(0.999985, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
