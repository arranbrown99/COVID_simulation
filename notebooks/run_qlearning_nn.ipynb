{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\euanf\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\euanf\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\euanf\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Keras and backend for neural networks\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "class NNFunctionApproximatorJointKeras():\n",
    "    \"\"\" A basic MLP neural network approximator and estimator using Keras     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, d_states, n_actions, nn_config, verbose=False):        \n",
    "        self.alpha = alpha \n",
    "        self.nn_config = nn_config      # determines the size of the hidden layer (if any)             \n",
    "        self.n_actions = n_actions        \n",
    "        self.d_states = d_states\n",
    "        self.verbose=verbose # Print debug information        \n",
    "        self.n_layers = len(nn_config)\n",
    "        self.model = self._build_model()  \n",
    "                        \n",
    "    def _huber_loss(self,y_true, y_pred, clip_delta=1.0):\n",
    "        \"\"\"\n",
    "        Huber loss (for use in Keras), see https://en.wikipedia.org/wiki/Huber_loss\n",
    "        The huber loss tends to provide more robust learning in RL settings where there are \n",
    "        often \"outliers\" before the functions has converged.\n",
    "        \"\"\"\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "#     def _build_model(self):\n",
    "#         # Neural Net for Deep-Q learning \n",
    "#         model = Sequential()\n",
    "        \n",
    "#         model.add(Dense(24, input_dim=4, activation=\"relu\"))\n",
    "#         model.add(Dense(24, activation=\"relu\"))\n",
    "#         model.add(Dense(4, input_dim=24, activation=\"relu\"))\n",
    "# #         print(model.output_shape)\n",
    "        \n",
    "# #         model.add(Input(shape=(4,)))\n",
    "# #         model.add(Dense(24, activation=\"relu\"))\n",
    "# #         model.add(Dense(24, activation=\"relu\"))\n",
    "# #         model.add(Dense(4, input_dim=24, activation=\"relu\"))\n",
    "# #         print(model.output_shape)\n",
    "        \n",
    "# #         model.add(Dense(units=24, input_dim=(4,), activation='relu'))       \n",
    "# #         model.add(Dense(units=24, input_dim=24, activation='relu'))        \n",
    "# #         model.add(Dense(self.n_actions, activation='sigmoid'))\n",
    "        \n",
    "#         model.compile(loss=self._huber_loss, # define a special loss function\n",
    "#                       optimizer=Adam(lr=self.alpha, clipnorm=10.)) # specify the optimiser, we clip the gradient of the norm which can make traning more robust\n",
    "#         return model\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning \n",
    "        model = Sequential()\n",
    "        for ilayer in self.nn_config:\n",
    "            model.add(Dense(ilayer, input_dim=self.d_states, activation='relu'))        \n",
    "        model.add(Dense(self.n_actions, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss, # define a special loss function\n",
    "                      optimizer=Adam(lr=self.alpha, clipnorm=10.)) # specify the optimiser, we clip the gradient of the norm which can make traning more robust\n",
    "        return model\n",
    "\n",
    " \n",
    "\n",
    "    def predict(self, s, a=None):              \n",
    "        if a==None:            \n",
    "            return self._predict_nn(s)\n",
    "        else:                        \n",
    "            return self._predict_nn(s)[a]\n",
    "        \n",
    "    def _predict_nn(self,state_hat):                          \n",
    "        \"\"\"\n",
    "        Predict the output of the neural netwwork (note: these can be vectors)\n",
    "        \"\"\"                \n",
    "        x = self.model.predict(state_hat)                                                    \n",
    "        return x\n",
    "  \n",
    "    def update(self, states, td_target):\n",
    "        self.model.fit(states, td_target, epochs=1, verbose=0) # take one gradient step usign Adam               \n",
    "        return\n",
    "\n",
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "\n",
    "d_states    = env.observation_space.shape[0]\n",
    "n_actions   = env.action_space.n\n",
    "alpha= 0.001          # learning rate/stepsize, 0.001 seems to be a good choice\n",
    "nn_config   = [24,24] # size of the hidden layers in the MLP [24,24 seems to be a good choice]\n",
    "# BATCH_SIZE  = 128     # numbe rof samples in a batch\n",
    "\n",
    "# print(d_states)\n",
    "# print(n_actions)\n",
    "nn = NNFunctionApproximatorJointKeras(alpha, d_states, n_actions, nn_config)\n",
    "\n",
    "\n",
    "epislon = 1\n",
    "rewards = []\n",
    "done = False\n",
    "state = env.reset()\n",
    "print(state.shape)\n",
    "state = np.reshape(state, [1, d_states]) # reshape to the a 1xd_state numpy array\n",
    "print(state.shape)\n",
    "\n",
    "# while not done:\n",
    "#     random_number = np.random.random()\n",
    "#     if random_number < epislon:\n",
    "#         #explore\n",
    "#         action = np.random.choice(n_actions)\n",
    "#     else:\n",
    "#         #exploit\n",
    "#         action = nn.predict(state)\n",
    "#         # get prediction here\n",
    "\n",
    "#     new_state, reward, done, i = env.step(action=action) # Q-learning\n",
    "#     new_state = np.reshape(new_state, [1, d_states])\n",
    "\n",
    "#     #update q table\n",
    "#     nn.update2(new_state)\n",
    "\n",
    "#     rewards.append(reward)\n",
    "#     state = new_state\n",
    "#     epsilon -= 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "Best total reward has been updated to -1.8109640552655963, for episode 0\n",
      "1\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "Best total reward has been updated to -1.3392901120820002, for episode 1\n",
      "2\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "3\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "4\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "5\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "6\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "Best total reward has been updated to -1.12820547386193, for episode 6\n",
      "7\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "8\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "9\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "10\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n",
      "11\n",
      "[[5.9996e+08 2.0000e+04 0.0000e+00 2.0000e+04]]\n"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'new_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Implement a replay buffer using the deque collection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)               \n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def pop(self):\n",
    "        return self.memory.pop()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    \n",
    "\n",
    "BATCH_SIZE  = 32     # number of samples in a batch\n",
    "REPLAY_MEMORY_SIZE = 1000   # size of the replay buffer\n",
    "\n",
    "def qlearning_nn(env):\n",
    "    memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "    n_actions = env.action_space.n\n",
    "    d_states = env.observation_space.shape[0]\n",
    "    alpha= 0.001          # learning rate/stepsize, 0.001 seems to be a good choice\n",
    "    nn_config   = [24,24] # size of the hidden layers in the MLP [24,24 seems to be a good choice]\n",
    "    num_episodes = 1000\n",
    "    epsilon = 1\n",
    "    epsilon_decay = 0.99995\n",
    "    discount_factor=0.95\n",
    "    \n",
    "    # Init the two networks\n",
    "    policy_network = NNFunctionApproximatorJointKeras(alpha, d_states, n_actions, nn_config)\n",
    "    target_network = NNFunctionApproximatorJointKeras(alpha, d_states, n_actions, nn_config)\n",
    "    target_network.model.set_weights(policy_network.model.get_weights())\n",
    "    \n",
    "    best_total_reward = -20\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        rewards = []\n",
    "        print(episode)\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, d_states])\n",
    "        print(state)\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            random_number = np.random.random()\n",
    "            if random_number < epislon:\n",
    "                #explore\n",
    "                action = np.random.choice(n_actions)\n",
    "            else:\n",
    "                #exploit\n",
    "                print(\"We are exploiting\")\n",
    "                action = nn.predict(state)[0]\n",
    "                action = np.argmax(action)\n",
    "\n",
    "            new_state, reward, done, i = env.step(action=action)\n",
    "            new_state = np.reshape(new_state, [1, d_states])\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            memory.push(state, action, new_state, reward)\n",
    "            \n",
    "            if len(memory) >= BATCH_SIZE:                         \n",
    "                # Fetch a batch from the replay buffer and extract as numpy arrays \n",
    "                transitions = memory.sample(BATCH_SIZE)            \n",
    "                batch = Transition(*zip(*transitions))                                \n",
    "                train_rewards = np.array(batch.reward)\n",
    "                train_states = np.array(batch.state)\n",
    "                train_new_state = np.array(batch.new_state)\n",
    "                train_actions = np.array(batch.action)\n",
    "                \n",
    "                q_values_for_current_state = policy_network.predict(train_states.reshape(BATCH_SIZE,d_states)) # predict current values for the given states\n",
    "                q_values_for_new_state     = target_network.predict(train_new_state.reshape(BATCH_SIZE,d_states))                    \n",
    "                q_values_for_current_state_tmp = train_rewards + discount_factor * np.amax(q_values_for_new_state,axis=1)                \n",
    "                q_values_for_current_state[ (np.arange(BATCH_SIZE), train_actions.reshape(BATCH_SIZE,).astype(int))] = q_values_for_current_state_tmp                                                                              \n",
    "                policy_network.update(train_states.reshape(BATCH_SIZE,d_states), q_values_for_current_state) # Update the function approximator \n",
    "       \n",
    "            state = new_state\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "            if done:\n",
    "                target_network.model.set_weights(policy_network.model.get_weights())\n",
    "                total_reward = np.sum(rewards)\n",
    "                all_rewards.append(total_reward)\n",
    "                if total_reward > best_total_reward:\n",
    "                    best_total_reward = total_reward\n",
    "                    print(\"Best total reward has been updated to \" + str(best_total_reward) + \", for episode \" + str(episode))\n",
    "                    \n",
    "    return all_rewards\n",
    "    \n",
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "r = qlearning_nn(env) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.num_of_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.q_table = QTable(initial=0, num_of_actions=self.num_of_actions) \n",
    "        \n",
    "        # hyper parameters\n",
    "        self.discount = 0.99 # gamma\n",
    "        self.learning_rate = 0.25 # step size, alpha\n",
    "        self.episodes = 2000\n",
    "        self.print_out_every_x_episodes = int(self.episodes/50)\n",
    "        \n",
    "        # hyper parameters for epsilon\n",
    "        self.initial_epsilon = 1 # initial\n",
    "        self.decrease_factor = (1/self.episodes)/1.25 # epsilon\n",
    "        self.decrease_factor = 0.00075\n",
    "        \n",
    "        print(\"Hyperparameter dump\")\n",
    "        print(\"----\")\n",
    "        print(\"Number Of Episodes = \" + str(self.episodes))\n",
    "        print(\"Print out every \" + str(self.print_out_every_x_episodes) + \" episodes\")\n",
    "        print(\"Learning Rate = \" + str(self.learning_rate))\n",
    "        print(\"Discount = \" + str(self.discount))\n",
    "        print(\"----\")\n",
    "        print(\"Initial Epsilon = \" + str(self.initial_epsilon))\n",
    "        print(\"Epsilon Decrease Factor = \" + str(self.decrease_factor))\n",
    "        print(\"----\")\n",
    "        print(\"Number of Bins to Discretise State = \" + str(self.number_bins))\n",
    "        print(\"----\")\n",
    "    \n",
    "    def run_all_episodes(self):\n",
    "        all_rewards = []\n",
    "        all_q_table_exploits = []\n",
    "        epislon = self.initial_epsilon # at the start only explore\n",
    "        \n",
    "        for episode in range(1, self.episodes + 1):\n",
    "            rewards, exploited_q_table = self.run_episode(epislon)\n",
    "            total_reward = np.sum(rewards)\n",
    "\n",
    "            if episode % self.print_out_every_x_episodes == 0:\n",
    "                print(\"Episode number: \" + str(episode) + \". Total reward in episode: \" + str(total_reward) + \". Episode executed with epsilon = \" + str(epislon))\n",
    "                print(\"Average total reward in last \" + str(self.print_out_every_x_episodes) + \" episodes: \" + str(np.mean(all_rewards[-self.print_out_every_x_episodes:])))\n",
    "                print(\"Average number of times we exploited q table in last \" + str(self.print_out_every_x_episodes) + \" episodes: \" + str(np.mean(all_q_table_exploits[-self.print_out_every_x_episodes:])))\n",
    "                print(\"-----\")\n",
    "            all_rewards.append(total_reward)\n",
    "            all_q_table_exploits.append(exploited_q_table)\n",
    "            epislon -= self.decrease_factor #hyperparameter\n",
    "            \n",
    "        return all_rewards\n",
    "    \n",
    "    def run_episode(self,epislon):\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        state = self.continous_to_discrete(state)\n",
    "        \n",
    "        exploited_q_table = 0\n",
    "        \n",
    "        while not done:\n",
    "            random_number = np.random.random()\n",
    "            if random_number < epislon:\n",
    "                #explore\n",
    "                action = np.random.choice(self.num_of_actions)\n",
    "            else:\n",
    "                #exploit\n",
    "                action = self.get_action(state)\n",
    "                exploited_q_table+=1\n",
    "                \n",
    "            new_state, reward, done, i = self.env.step(action=action) # Q-learning\n",
    "            new_state = self.continous_to_discrete(new_state)\n",
    "            \n",
    "            #update q table\n",
    "            self.update_q_table(state,new_state,action,reward)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            state = new_state\n",
    "        return (rewards, exploited_q_table)\n",
    "    \n",
    "    def update_q_table(self,state,new_state,action,reward):\n",
    "        #target\n",
    "        #max of a' given the \n",
    "        max_a_prime = np.max(self.q_table.get_actions(new_state))\n",
    "        target = reward + (self.discount*max_a_prime)\n",
    "        \n",
    "        #compute difference\n",
    "        action_value = self.q_table.get_action_value(state,action)\n",
    "        difference = target - action_value\n",
    "        \n",
    "        #take a small step in the delta direction\n",
    "        new_q = action_value + (self.learning_rate * difference)\n",
    "        \n",
    "        self.q_table.set_action_value(state,action,new_q)\n",
    "        \n",
    "    \n",
    "    def get_action(self,state):\n",
    "        #exploit the q table\n",
    "        actions = self.q_table.get_actions(state)\n",
    "        action = np.argmax(self.q_table.get_actions(state))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter dump\n",
      "----\n",
      "Number Of Episodes = 2000\n",
      "Print out every 40 episodes\n",
      "Learning Rate = 0.25\n",
      "Discount = 0.99\n",
      "----\n",
      "Initial Epsilon = 1\n",
      "Epsilon Decrease Factor = 0.0004\n",
      "----\n",
      "Number of Bins to Discretise State = 20\n",
      "----\n",
      "Episode number: 40. Total reward in episode: -2.0787121598145073. Episode executed with epsilon = 0.9844000000000017\n",
      "Average total reward in last 40 episodes: -1.6005948167025672\n",
      "Average number of times we exploited q table in last 40 episodes: 0.4358974358974359\n",
      "-----\n",
      "Episode number: 80. Total reward in episode: -1.8484907226700567. Episode executed with epsilon = 0.9684000000000035\n",
      "Average total reward in last 40 episodes: -1.6368228308587138\n",
      "Average number of times we exploited q table in last 40 episodes: 1.25\n",
      "-----\n",
      "Episode number: 120. Total reward in episode: -1.625091768120587. Episode executed with epsilon = 0.9524000000000052\n",
      "Average total reward in last 40 episodes: -1.714146345759815\n",
      "Average number of times we exploited q table in last 40 episodes: 2.15\n",
      "-----\n",
      "Episode number: 160. Total reward in episode: -1.6720150780285241. Episode executed with epsilon = 0.936400000000007\n",
      "Average total reward in last 40 episodes: -1.643044023426743\n",
      "Average number of times we exploited q table in last 40 episodes: 3.1\n",
      "-----\n",
      "Episode number: 200. Total reward in episode: -1.7430739873441394. Episode executed with epsilon = 0.9204000000000088\n",
      "Average total reward in last 40 episodes: -1.6787367780643017\n",
      "Average number of times we exploited q table in last 40 episodes: 3.55\n",
      "-----\n",
      "Episode number: 240. Total reward in episode: -2.038154593839459. Episode executed with epsilon = 0.9044000000000105\n",
      "Average total reward in last 40 episodes: -1.633307142368114\n",
      "Average number of times we exploited q table in last 40 episodes: 4.4\n",
      "-----\n",
      "Episode number: 280. Total reward in episode: -1.9211218736407294. Episode executed with epsilon = 0.8884000000000123\n",
      "Average total reward in last 40 episodes: -1.7076036000547723\n",
      "Average number of times we exploited q table in last 40 episodes: 5.375\n",
      "-----\n",
      "Episode number: 320. Total reward in episode: -2.0657431662695873. Episode executed with epsilon = 0.872400000000014\n",
      "Average total reward in last 40 episodes: -1.7059798128947619\n",
      "Average number of times we exploited q table in last 40 episodes: 5.775\n",
      "-----\n",
      "Episode number: 360. Total reward in episode: -1.8439851107930487. Episode executed with epsilon = 0.8564000000000158\n",
      "Average total reward in last 40 episodes: -1.6572170588581536\n",
      "Average number of times we exploited q table in last 40 episodes: 6.375\n",
      "-----\n",
      "Episode number: 400. Total reward in episode: -1.6119523220150547. Episode executed with epsilon = 0.8404000000000176\n",
      "Average total reward in last 40 episodes: -1.7183188838759378\n",
      "Average number of times we exploited q table in last 40 episodes: 8.05\n",
      "-----\n",
      "Episode number: 440. Total reward in episode: -1.6287852741717563. Episode executed with epsilon = 0.8244000000000193\n",
      "Average total reward in last 40 episodes: -1.5804567966869216\n",
      "Average number of times we exploited q table in last 40 episodes: 9.05\n",
      "-----\n",
      "Episode number: 480. Total reward in episode: -1.7453379151095143. Episode executed with epsilon = 0.8084000000000211\n",
      "Average total reward in last 40 episodes: -1.6262193800667801\n",
      "Average number of times we exploited q table in last 40 episodes: 9.45\n",
      "-----\n",
      "Episode number: 520. Total reward in episode: -1.780355331697039. Episode executed with epsilon = 0.7924000000000229\n",
      "Average total reward in last 40 episodes: -1.6490972158690493\n",
      "Average number of times we exploited q table in last 40 episodes: 10.725\n",
      "-----\n",
      "Episode number: 560. Total reward in episode: -1.945031828670969. Episode executed with epsilon = 0.7764000000000246\n",
      "Average total reward in last 40 episodes: -1.5982805582760218\n",
      "Average number of times we exploited q table in last 40 episodes: 11.875\n",
      "-----\n",
      "Episode number: 600. Total reward in episode: -1.913321592544035. Episode executed with epsilon = 0.7604000000000264\n",
      "Average total reward in last 40 episodes: -1.6722253720262916\n",
      "Average number of times we exploited q table in last 40 episodes: 12.175\n",
      "-----\n",
      "Episode number: 640. Total reward in episode: -1.4772868134973236. Episode executed with epsilon = 0.7444000000000282\n",
      "Average total reward in last 40 episodes: -1.6341412865628846\n",
      "Average number of times we exploited q table in last 40 episodes: 13.85\n",
      "-----\n",
      "Episode number: 680. Total reward in episode: -1.4171416820177039. Episode executed with epsilon = 0.7284000000000299\n",
      "Average total reward in last 40 episodes: -1.630268931322714\n",
      "Average number of times we exploited q table in last 40 episodes: 13.35\n",
      "-----\n",
      "Episode number: 720. Total reward in episode: -1.7126336715914787. Episode executed with epsilon = 0.7124000000000317\n",
      "Average total reward in last 40 episodes: -1.602850210912144\n",
      "Average number of times we exploited q table in last 40 episodes: 14.225\n",
      "-----\n",
      "Episode number: 760. Total reward in episode: -1.5153910106314636. Episode executed with epsilon = 0.6964000000000334\n",
      "Average total reward in last 40 episodes: -1.6155770387549766\n",
      "Average number of times we exploited q table in last 40 episodes: 15.075\n",
      "-----\n",
      "Episode number: 800. Total reward in episode: -1.351756875716596. Episode executed with epsilon = 0.6804000000000352\n",
      "Average total reward in last 40 episodes: -1.630281392376574\n",
      "Average number of times we exploited q table in last 40 episodes: 15.65\n",
      "-----\n",
      "Episode number: 840. Total reward in episode: -1.7454535606604293. Episode executed with epsilon = 0.664400000000037\n",
      "Average total reward in last 40 episodes: -1.6706355859149753\n",
      "Average number of times we exploited q table in last 40 episodes: 17.55\n",
      "-----\n",
      "Episode number: 880. Total reward in episode: -1.7744765909152527. Episode executed with epsilon = 0.6484000000000387\n",
      "Average total reward in last 40 episodes: -1.6448101290883197\n",
      "Average number of times we exploited q table in last 40 episodes: 17.425\n",
      "-----\n",
      "Episode number: 920. Total reward in episode: -1.7258515544887385. Episode executed with epsilon = 0.6324000000000405\n",
      "Average total reward in last 40 episodes: -1.6659116587755576\n",
      "Average number of times we exploited q table in last 40 episodes: 19.55\n",
      "-----\n",
      "Episode number: 960. Total reward in episode: -1.4661085487016066. Episode executed with epsilon = 0.6164000000000422\n",
      "Average total reward in last 40 episodes: -1.6021265065907708\n",
      "Average number of times we exploited q table in last 40 episodes: 18.8\n",
      "-----\n",
      "Episode number: 1000. Total reward in episode: -1.8256915158258813. Episode executed with epsilon = 0.600400000000044\n",
      "Average total reward in last 40 episodes: -1.627582300611472\n",
      "Average number of times we exploited q table in last 40 episodes: 20.9\n",
      "-----\n",
      "Episode number: 1040. Total reward in episode: -1.7699553582639727. Episode executed with epsilon = 0.5844000000000458\n",
      "Average total reward in last 40 episodes: -1.614693428051691\n",
      "Average number of times we exploited q table in last 40 episodes: 21.875\n",
      "-----\n",
      "Episode number: 1080. Total reward in episode: -1.4034792278782362. Episode executed with epsilon = 0.5684000000000475\n",
      "Average total reward in last 40 episodes: -1.5972800862372896\n",
      "Average number of times we exploited q table in last 40 episodes: 21.975\n",
      "-----\n",
      "Episode number: 1120. Total reward in episode: -1.4069654973807253. Episode executed with epsilon = 0.5524000000000493\n",
      "Average total reward in last 40 episodes: -1.6251906299390655\n",
      "Average number of times we exploited q table in last 40 episodes: 23.35\n",
      "-----\n",
      "Episode number: 1160. Total reward in episode: -1.6640456512445085. Episode executed with epsilon = 0.5364000000000511\n",
      "Average total reward in last 40 episodes: -1.5683811045594869\n",
      "Average number of times we exploited q table in last 40 episodes: 23.25\n",
      "-----\n",
      "Episode number: 1200. Total reward in episode: -1.4484530077369733. Episode executed with epsilon = 0.5204000000000528\n",
      "Average total reward in last 40 episodes: -1.6047866823031343\n",
      "Average number of times we exploited q table in last 40 episodes: 23.55\n",
      "-----\n",
      "Episode number: 1240. Total reward in episode: -1.4691046337904017. Episode executed with epsilon = 0.5044000000000546\n",
      "Average total reward in last 40 episodes: -1.5661593053042207\n",
      "Average number of times we exploited q table in last 40 episodes: 25.35\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number: 1280. Total reward in episode: -1.826881864986881. Episode executed with epsilon = 0.48840000000005473\n",
      "Average total reward in last 40 episodes: -1.6288770970265027\n",
      "Average number of times we exploited q table in last 40 episodes: 25.3\n",
      "-----\n",
      "Episode number: 1320. Total reward in episode: -1.7974512406754635. Episode executed with epsilon = 0.4724000000000543\n",
      "Average total reward in last 40 episodes: -1.5803107731171218\n",
      "Average number of times we exploited q table in last 40 episodes: 27.625\n",
      "-----\n",
      "Episode number: 1360. Total reward in episode: -1.8257774564501008. Episode executed with epsilon = 0.4564000000000538\n",
      "Average total reward in last 40 episodes: -1.5734463367990428\n",
      "Average number of times we exploited q table in last 40 episodes: 28.25\n",
      "-----\n",
      "Episode number: 1400. Total reward in episode: -1.8668698225426226. Episode executed with epsilon = 0.44040000000005336\n",
      "Average total reward in last 40 episodes: -1.5657419735834583\n",
      "Average number of times we exploited q table in last 40 episodes: 29.5\n",
      "-----\n",
      "Episode number: 1440. Total reward in episode: -1.4636877938628559. Episode executed with epsilon = 0.4244000000000529\n",
      "Average total reward in last 40 episodes: -1.533256922175652\n",
      "Average number of times we exploited q table in last 40 episodes: 29.775\n",
      "-----\n",
      "Episode number: 1480. Total reward in episode: -1.5772399889057158. Episode executed with epsilon = 0.40840000000005244\n",
      "Average total reward in last 40 episodes: -1.6005631346089\n",
      "Average number of times we exploited q table in last 40 episodes: 31.325\n",
      "-----\n",
      "Episode number: 1520. Total reward in episode: -1.6910296869526606. Episode executed with epsilon = 0.392400000000052\n",
      "Average total reward in last 40 episodes: -1.5456338732315016\n",
      "Average number of times we exploited q table in last 40 episodes: 30.875\n",
      "-----\n",
      "Episode number: 1560. Total reward in episode: -1.3628043832533523. Episode executed with epsilon = 0.3764000000000515\n",
      "Average total reward in last 40 episodes: -1.592667209560243\n",
      "Average number of times we exploited q table in last 40 episodes: 30.7\n",
      "-----\n",
      "Episode number: 1600. Total reward in episode: -1.4605960019091855. Episode executed with epsilon = 0.36040000000005107\n",
      "Average total reward in last 40 episodes: -1.5528311656419258\n",
      "Average number of times we exploited q table in last 40 episodes: 33.625\n",
      "-----\n",
      "Episode number: 1640. Total reward in episode: -1.6111034295626583. Episode executed with epsilon = 0.3444000000000506\n",
      "Average total reward in last 40 episodes: -1.5960081633423535\n",
      "Average number of times we exploited q table in last 40 episodes: 32.8\n",
      "-----\n",
      "Episode number: 1680. Total reward in episode: -1.6962795721483845. Episode executed with epsilon = 0.32840000000005015\n",
      "Average total reward in last 40 episodes: -1.5488517529148647\n",
      "Average number of times we exploited q table in last 40 episodes: 34.45\n",
      "-----\n",
      "Episode number: 1720. Total reward in episode: -1.4894893150359285. Episode executed with epsilon = 0.3124000000000497\n",
      "Average total reward in last 40 episodes: -1.510551362132866\n",
      "Average number of times we exploited q table in last 40 episodes: 35.15\n",
      "-----\n",
      "Episode number: 1760. Total reward in episode: -1.8764926488650726. Episode executed with epsilon = 0.29640000000004924\n",
      "Average total reward in last 40 episodes: -1.5523354316301359\n",
      "Average number of times we exploited q table in last 40 episodes: 36.1\n",
      "-----\n",
      "Episode number: 1800. Total reward in episode: -1.4641983434728587. Episode executed with epsilon = 0.2804000000000488\n",
      "Average total reward in last 40 episodes: -1.5108320589341846\n",
      "Average number of times we exploited q table in last 40 episodes: 36.525\n",
      "-----\n",
      "Episode number: 1840. Total reward in episode: -1.7620943504165933. Episode executed with epsilon = 0.2644000000000483\n",
      "Average total reward in last 40 episodes: -1.5188715194898108\n",
      "Average number of times we exploited q table in last 40 episodes: 38.025\n",
      "-----\n",
      "Episode number: 1880. Total reward in episode: -1.353484486107406. Episode executed with epsilon = 0.24840000000004786\n",
      "Average total reward in last 40 episodes: -1.5115529323144299\n",
      "Average number of times we exploited q table in last 40 episodes: 39.225\n",
      "-----\n",
      "Episode number: 1920. Total reward in episode: -1.5744653360964433. Episode executed with epsilon = 0.2324000000000474\n",
      "Average total reward in last 40 episodes: -1.5415394714828228\n",
      "Average number of times we exploited q table in last 40 episodes: 39.475\n",
      "-----\n",
      "Episode number: 1960. Total reward in episode: -1.4913168210796628. Episode executed with epsilon = 0.21640000000004694\n",
      "Average total reward in last 40 episodes: -1.5135783803241356\n",
      "Average number of times we exploited q table in last 40 episodes: 40.5\n",
      "-----\n",
      "Episode number: 2000. Total reward in episode: -1.4436827592970023. Episode executed with epsilon = 0.20040000000004649\n",
      "Average total reward in last 40 episodes: -1.4656057312474995\n",
      "Average number of times we exploited q table in last 40 episodes: 40.55\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "agent = QLearningAgent(env)\n",
    "rewards = agent.run_all_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(agent, rewards):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    axes[1].plot(rewards);\n",
    "    axes[1].set_xlabel('episode')\n",
    "    axes[1].set_ylabel('total reward r(t)')\n",
    "    \n",
    "plot(agent, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
