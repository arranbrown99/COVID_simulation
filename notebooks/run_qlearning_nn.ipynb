{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full window width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "os.chdir('..')\n",
    "import virl\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras and backend for neural networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NNFunctionApproximatorJointKeras():\n",
    "    \"\"\" A basic MLP neural network approximator and estimator using Keras     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, d_states, n_actions, nn_config, verbose=False):        \n",
    "        self.alpha = alpha \n",
    "        self.nn_config = nn_config      # determines the size of the hidden layer (if any)             \n",
    "        self.n_actions = n_actions        \n",
    "        self.d_states = d_states\n",
    "        self.verbose=verbose # Print debug information        \n",
    "        self.n_layers = len(nn_config)\n",
    "        self.model = self._build_model()  \n",
    "                        \n",
    "    def _huber_loss(self,y_true, y_pred, clip_delta=1.0):\n",
    "        \"\"\"\n",
    "        Huber loss (for use in Keras), see https://en.wikipedia.org/wiki/Huber_loss\n",
    "        The huber loss tends to provide more robust learning in RL settings where there are \n",
    "        often \"outliers\" before the functions has converged.\n",
    "        \"\"\"\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning \n",
    "        model = Sequential()\n",
    "        for ilayer in self.nn_config:\n",
    "            model.add(Dense(ilayer, input_dim=self.d_states, activation='relu'))        \n",
    "        model.add(Dense(self.n_actions, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss, # define a special loss function\n",
    "                      optimizer=Adam(lr=self.alpha, clipnorm=10.)) # specify the optimiser, we clip the gradient of the norm which can make traning more robust\n",
    "        return model\n",
    "\n",
    "    def predict(self, s, a=None):              \n",
    "        if a==None:            \n",
    "            return self._predict_nn(s)\n",
    "        else:                        \n",
    "            return self._predict_nn(s)[a]\n",
    "        \n",
    "    def _predict_nn(self,state_hat):                          \n",
    "        \"\"\"\n",
    "        Predict the output of the neural netwwork (note: these can be vectors)\n",
    "        \"\"\"                \n",
    "        x = self.model.predict(state_hat)                                                    \n",
    "        return x\n",
    "  \n",
    "    def update(self, states, td_target):\n",
    "        self.model.fit(states, td_target, epochs=1, verbose=0) # take one gradient step usign Adam               \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'new_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Implement a replay buffer using the deque collection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)               \n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def pop(self):\n",
    "        return self.memory.pop()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  batch_size - number of samples in a batch\n",
    "#  replay_memory_size - size of the replay buffer\n",
    "#  nn_config - size of the hidden layers in the MLP [24,24 seems to be a good choice]\n",
    "def qlearning_nn(batch_size=80, replay_memory_size=600, nn_config=[24,24], num_episodes=100, epsilon_decay=0.9997, output=True, evaluation=False):\n",
    "    env = virl.Epidemic(stochastic=False, noisy=False)\n",
    "\n",
    "    memory = ReplayMemory(replay_memory_size)\n",
    "    n_actions = env.action_space.n\n",
    "    d_states = env.observation_space.shape[0]\n",
    "    alpha= 0.001          # learning rate/stepsize, 0.001 seems to be a good choice\n",
    "    epsilon = 1\n",
    "    discount_factor = 0.95\n",
    "    \n",
    "    # Init the two networks\n",
    "    policy_network = NNFunctionApproximatorJointKeras(alpha, d_states, n_actions, nn_config)\n",
    "    target_network = NNFunctionApproximatorJointKeras(alpha, d_states, n_actions, nn_config)\n",
    "    target_network.model.set_weights(policy_network.model.get_weights())\n",
    "    \n",
    "    best_total_reward = -20\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        rewards = []\n",
    "        \n",
    "        if evaluation:\n",
    "            if episode == int(num_episodes/4):\n",
    "                print(\"--> 25% complete\")\n",
    "            if episode == int(num_episodes/2):\n",
    "                print(\"--> 50% complete\")\n",
    "            if episode == int((num_episodes/4)*3):\n",
    "                print(\"--> 75% complete\")\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, d_states])\n",
    "        exploits = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            random_number = np.random.random()\n",
    "            if random_number < epsilon and episode < (num_episodes-5):\n",
    "                #explore\n",
    "                action = np.random.choice(n_actions)\n",
    "            else:\n",
    "                #exploit\n",
    "                exploits += 1\n",
    "                action = policy_network.predict(state)[0]\n",
    "                action = np.argmax(action)\n",
    "\n",
    "            new_state, reward, done, i = env.step(action=action)\n",
    "            new_state = np.reshape(new_state, [1, d_states])\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            memory.push(state, action, new_state, reward)\n",
    "            \n",
    "            if len(memory) >= batch_size:                         \n",
    "                # Fetch a batch from the replay buffer and extract as numpy arrays \n",
    "                transitions = memory.sample(batch_size)            \n",
    "                batch = Transition(*zip(*transitions))                                \n",
    "                train_rewards = np.array(batch.reward)\n",
    "                train_states = np.array(batch.state)\n",
    "                train_new_state = np.array(batch.new_state)\n",
    "                train_actions = np.array(batch.action)\n",
    "                \n",
    "                q_values_for_current_state = policy_network.predict(train_states.reshape(batch_size,d_states)) # predict current values for the given states\n",
    "                q_values_for_new_state     = target_network.predict(train_new_state.reshape(batch_size,d_states))                    \n",
    "                q_values_for_current_state_tmp = train_rewards + discount_factor * np.amax(q_values_for_new_state,axis=1)                \n",
    "                q_values_for_current_state[ (np.arange(batch_size), train_actions.reshape(batch_size,).astype(int))] = q_values_for_current_state_tmp                                                                              \n",
    "                policy_network.update(train_states.reshape(batch_size,d_states), q_values_for_current_state) # Update the function approximator \n",
    "       \n",
    "            state = new_state\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "            if done:\n",
    "                target_network.model.set_weights(policy_network.model.get_weights())\n",
    "                total_reward = np.sum(rewards)\n",
    "                all_rewards.append(total_reward)\n",
    "                if output:\n",
    "                    print(\"Episode = \" + str(episode) + \". Epsilon = \" + str(epsilon) + \". Num Exploits = \" + str(exploits) + \". Total Reward = \" + str(total_reward))\n",
    "                \n",
    "                if total_reward > best_total_reward:\n",
    "                    best_total_reward = total_reward\n",
    "                    if output:\n",
    "                        print(\"Best total reward has been updated to \" + str(best_total_reward) + \", for episode \" + str(episode))\n",
    "    \n",
    "    average_of_last_twenty_rewards = np.mean(all_rewards[-20:]) if len(all_rewards) > 20 else np.mean(all_rewards)\n",
    "    return (all_rewards, average_of_last_twenty_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_epsilon_after(decay, after_episodes):\n",
    "    return (decay**52)**after_episodes\n",
    "\n",
    "# def get_ideal_buffer_size_for_fifty_episodes():\n",
    "#     return (50 * 52)/6.5\n",
    "\n",
    "print(check_epsilon_after(0.9988005, 25))\n",
    "print(check_epsilon_after(0.9994, 50))\n",
    "print(check_epsilon_after(0.9997, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_config = [24,24]\n",
    "num_episodes = 50\n",
    "epsilon_decay = 0.9994\n",
    "print(\"This setting will result in epsilon = \" + str(check_epsilon_after(epsilon_decay, num_episodes)) + \" after \" + str(num_episodes) + \" episodes.\")\n",
    "# 0.21\n",
    "\n",
    "replay_size = 400\n",
    "all_values = [60, 70, 80]\n",
    "\n",
    "eval_all_rewards = []\n",
    "eval_average_of_last_twenty_rewards = []\n",
    "for batch_size in all_values:\n",
    "    print(\"Starting evaluation with batch size \" + str(batch_size))\n",
    "    all_rewards, average_of_last_twenty_rewards = qlearning_nn(batch_size=batch_size, replay_memory_size=replay_size, nn_config=nn_config, num_episodes=num_episodes, epsilon_decay=epsilon_decay, output=False, evaluation=True)\n",
    "    print(\"Average of last twenty rewards = \" + str(average_of_last_twenty_rewards))\n",
    "    eval_all_rewards.append(all_rewards)\n",
    "    eval_average_of_last_twenty_rewards.append(average_of_last_twenty_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(all_values, eval_average_of_last_twenty_rewards)\n",
    "plt.xlabel(\"Replay buffer size\")\n",
    "plt.ylabel(\"Average reward over last twenty episodes\")\n",
    "plt.title(\"\")\n",
    "\n",
    "# print(all_values)\n",
    "# print(eval_all_rewards)\n",
    "# print(eval_average_of_last_twenty_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(all_rewards)\n",
    "# plt.xlabel(\"Episodes\")\n",
    "# plt.ylabel(\"Total Rewards\")\n",
    "# print(np.sum(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
